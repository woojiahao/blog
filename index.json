[{"categories":null,"contents":"I got my iPad Air 4 a few months ago and have been wanting to test the AirPlay functionality on it. However, as I am on Manjaro Linux - a Linux distribution based on Arch Linux, I was unable to get the native support that MacOS has.\nI stumbled upon UxPlay as a potential solution for this problem.\nUxPlay is an AirPlay Unix mirroring server that acts like an AppleTV for screen-mirroring on the machine that is running the server. It only works on UNIX systems.\nThe README is quite verbose and only contains the package names for Debian, Red Hat, Fedora, CentOS, OpenSUSE, FreeBSD distros.\nSo, this guide aims to help install UxPlay on Arch/Manjaro.\nInstallation Install the necessary dependencies.\n1 2 3 4 5 6 7 yay -S cmake pkgconf yay -S openssl libplist yay -S avahi gstreamer gst-plugins-base gst-libav gst-plugins-bad yay -S gstreamer-vaapi yay -S libx11 # If you are using Manjaro, you can should also install the manjaro-gstreamer yay -S manjaro-gstreamer Clone the UxPlay repository.\n1 2 git clone https://github.com/antimof/UxPlay.git cd UxPlay Build and install the server.\n1 2 3 cmake . make sudo make install UxPlay will be installed to /usr/local/bin/uxplay.\nRun the server.\n1 sudo /usr/local/bin/uxplay That\u0026rsquo;s all there is to this!\nOptimizing for GoodNotes 5 One of the main uses I have for AirPlay is to annotate and write notes in GoodNotes 5 without having the worry about the size constraint. Typically, I would use a 50:50 layout, where one side hosts my annotated notes (like a textbook) and the other is a notebook for my notes.\nHowever, with AirPlay, we can configure GoodNotes 5 to mirror only the notes side (displaying it on screen). The other side will remain the same. This way, the layout can be 25:75 without sacrificing the readability of the notes as the notes are displayed on a bigger screen while still being controlled via the 25% on the iPad.\n","date":"Feb 01","permalink":"https://blog.woojiahao.com/post/uxplay-arch-linux/","tags":["tutorial","guide","unix","arch","archlinux","manjaro","uxplay","airplay","ios","ipad"],"title":"Installing UxPlay on Arch Linux"},{"categories":null,"contents":"This open-source deep dive has been split into two parts! The first part covers the prerequisite knowledge that would be good to know when trying to understand the inner workings of Broadway. The second part is an in-depth analysis of the implementation of various features of Broadway.\nThis is the first part of the deep dive and the following topics will be covered:\nA brief introduction to what Broadway is Message queues Concurrency in Elixir Producer/consumer model \u0026amp; GenStage Architecture of a Broadway pipeline Construction of producer \u0026amp; processor components If you wish to jump right into the meat of Broadway, you can find the second part here!.\nAct 1, Scene 1 You have just received your latest feature to work on and it is to build a system that receives transaction information from a message queue, maps the customer code in this transaction information to the customer\u0026rsquo;s information, and stores this collective information in a separate database to be queried for customer transaction analysis. Your boss has developed an obsession with Elixir recently and is now pushing for every project to use it. Gasp.\nYou start researching for libraries that can do exactly that and stumble upon Broadway.\n\u0026hellip;build [concurrent] and [multi-stage] [data ingestion] and [data processing] [pipelines]\u0026hellip;\nOh boy\u0026hellip; that — that is a mouthful\u0026hellip; Let\u0026rsquo;s break it down, shall we?\nconcurrent - having two or more computations in progress at the same time; in progress meaning that they do not have to be executed at the same time ( definition here) multi-stage - successive operating stages ( definition here) data ingestion - process of moving data from one source to a destination for further storage and analysis ( definition here) data processing - conversion of data into a usable and desirable form ( definition here) pipelines - series of data processing elements ( definition here) In essence, Broadway builds systems that behave like factory assembly lines. Raw materials (data) is fed into the assembly line (Broadway pipeline) which is then pieced together to create the end product or other components used in the final product. The factory has multiple identical assembly lines running so raw material can be fed into any of these lines to be worked on.\nFor your use case, the flow of data will look something like this:\nSo how does Broadway achieve all of this?\nLights! Camera! Action! Before understanding the internals of Broadway, we should establish some basic knowledge of the technologies we will be using so that we won\u0026rsquo;t be headless chickens running into this.\nBroadway revolve around the following concepts:\nmessage queues concurrency in Elixir What are message queues? Note! While Broadway can integrate with many types of data sources, the core examples given in the project focus on message queues as the primary data source.\nMessage queues are like containers that hold sequences of work objects — called messages — that are to be consumed and processed. It aids with building asynchronous modular and concurrent systems.\nMessages are created and delivered to these queues by producers and taken from these queues for processing by ** consumers.** These messages can vary from something as simple as plain information to more complex structures like requests or — in our case — transaction information.\nMessage queues are useful for decentralising the communication mechanism of large systems by acting as a medium for exchanging events between systems which allows for systems to be easily scaled and distributed.\nThis is a reduced explanation of what a message queue is and what it is capable of. For more information about message queues, the Amazon documentation and this blog post by CloudAMQP are good places to start.\nConcurrency in Elixir Broadway relies heavily on concurrency in Elixir. The topology (architecture) of a pipeline is built on top of processes and many of the features are achieved using the robust concurrency model of Elixir. So what exactly is the concurrency model in Elixir?\nElixir employs the actor concurrency model. In this model, actors are defined as self-isolated units of processing. In Elixir, these actors are called processes and they are managed by the Erlang VM . Elixir code is run in each process and a default/main process is akin to that of the main thread in other concurrency models.\nEach process communicates via asynchronous message passing. Think of a process as a mailbox of sorts; it has a \u0026ldquo;bin\u0026rdquo; to receive incoming messages and it possess an \u0026ldquo;address\u0026rdquo; for other processes to identify it by.\nThe unique aspect of this model is the lack of shared mutable state that other concurrency models rely on. Rather, state is exclusive to each process.\nIn order for the state of a process to be altered, the owner process must make the alteration either on request or internally due to certain changes.\nThe topic of concurrency in Elixir is vast and Elixir provides many other features surrounding its concurrency model such as GenServer. This section is a short preview of what the actor concurrency model and concurrency in Elixir is all about. For more information, you can refer to this thesis paper and the Wikipedia article talking about the actor concurrency model and the official documentation and this tutorial on OTP in Elixir for more examples of concurrency in Elixir.\nCue the producer/consumer model Using the actor concurrency model as a foundation, another concurrency pattern can be modelled in Elixir — the producer/consumer model.\nThis model aims to allow for decoupled data production and consumption by setting up two separate processes to handle each task — effectively creating a logical separation of concerns.\nHowever, the producer/consumer model faces a critical issue — what happens if the producer generates excessive messages for the consumer? The consumer will be overwhelmed and will eventually fail trying to keep up with processing that many messages. This is where back pressure comes into play.\nBack pressure is a control mechanism for how much a producer should emit based on consumer demand, consumer message buffering, or limited sampling\nBack pressure avoids the problem of overloading the consumer with messages by applying one of or a combination of the three methods mentioned above (more information in the link here).\nThe next frontier (of concurrency): GenStage Seeing the value of having a standard implementation for the producer/consumer model, the Elixir team decided to develop exactly that.\nGenStage is a specification for exchanging events between producers and consumers with back pressure between Elixir processes\nProducers emit events to consumers for processing. The events can be of any structure.\nThe control mechanism used is a demand system. Consumers inform producers of how many events they can handle (demand) and producers emits no more than the demanded amount. This ensures that the consumers are capable of handling the events emitted.\nProducer-consumers behave like both producers and consumers. They are used to perform transformations on events emitted by the producer before they are emitted to the consumer.\nSimilar to GenServer, stages in GenStage exchange events through callbacks.\nWhen a demand is handled — i.e. producer emits events and demanding consumer handles these events — another demand is made, creating a cycle where both stages are always working - ideally.\nGenStage is a powerful tool in an Elixir developer\u0026rsquo;s arsenal. More information can be found in the official announcement where a little bit of history of how GenStage came to be was discussed and in a talk by José Valim — creator of Elixir.\nWith a better grasp of the overarching concepts used in Broadway, we can finally discuss what Broadway is all about and how it does what it does!\nPipeline architecture It is at this juncture where it would be important to clarify the term \u0026ldquo;producer\u0026rdquo;. In both message queues and GenStage, a producer is a creator of messages or events. However, in Broadway, a producer is both a consumer of messages and an emitter of events.\nFor the rest of the article, the following definitions for the following terminology will be used:\nproducer — producer of events in Broadway message — message in a message queue or any other data source event — GenStage events When messages are consumed by the producer, they will be transformed into events with a fixed structure defined by Broadway.\nEach component is a separate process and they are dynamically generated as different topologies (architectures) can be designed. The order of initialisation for a typical pipeline looks something like this:\nThe producers and processors are both created using interesting conventions that is will be explored now. Other components will be discussed later on as they tie into other features Broadway has.\nHow it\u0026rsquo;s made: Producers Producers are built using a pattern similar to the strategy pattern but modified to integrate with the concurrency system in Elixir.\nDifferent data sources require different methods of establishing connections and receiving messages. Thus, we break up the producer process into two modules — ProducerStage defines the behavior for enforcing the rate limit while a dynamically loaded module defines the behavior for establishing a connection to the data source and receiving messages.\nProducerStage assumes that the dynamically loaded module contains the typical GenStage callbacks like handle_call and handle_demand and uses them for things like rate limiting.\nThe ProducerStage behaves as the context while the dynamic module behaves as the strategy. The dynamic module adopts the Producer module — which defines two callbacks for managing the overall producer life-cycle.\nTo load the module dynamically, the module name is passed to ProducerStage as an argument. To keep the producer as a single process, we call the init function of the module directly when initialising the ProducerStage. This way, the module will initialise under the newly spawned process for ProducerStage rather than spawning an entirely new process.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @impl true def init({args, index}) do {module, arg} = args[:module] # ... state = %{ module: module, module_state: nil, transformer: transformer, consumers: [], rate_limiting: rate_limiting_state } # Calling the init function of the dynamically loaded module case module.init(arg) do # ... end end When start_link is called, a new process is spawned first before the init function is called under the new process.\nThis is done as certain message queue providers like RabbitMQ attach active listeners to the calling process so spawning a separate process for this would mean having to manage two separate processes for a producer.\nHow it\u0026rsquo;s made: Processors Processors are created using a concept similar to inheritance in object-oriented programming. This idea comes from the need to standardise the subscription logic of producer-consumers and consumers.\nWhen a processor is started using start_link, a process of the Subscriber module is started with the current processor module passed as a argument.\n1 2 3 4 5 6 7 8 9 def start_link(args, stage_options) do Broadway.Topology.Subscriber.start_link( __MODULE__, args[:producers], args, Keyword.take(args[:processor_config], [:min_demand, :max_demand]), stage_options ) end The current module is initialised in the Subscriber process through init.\n1 2 3 4 5 6 @impl true def init({module, names, options, subscription_options}) do {type, state, init_options} = module.init(options) # ... end Other producer-consumers and consumers like batcher and batch processors also use this pattern to create their respective GenStage stages.\nA separation of concern is achieved using this pattern. The processor is responsible for event handling while the subscriber handles the subscription logic.\nThat\u0026rsquo;s a basic rundown of the concepts underpinning Broadway. While it may not be a complete and intensive explanation of everything, hopefully it is able to provide some clarity. In the next part, we will be exploring how features in Broadway have been implemented!\nHop on over to the second part here!\nOpen-source Deep Dive is a series where I pick apart open-source projects to explain the underlying concepts that power these projects and share my findings about the project!\n","date":"Apr 12","permalink":"https://blog.woojiahao.com/post/odd-broadway-1/","tags":["Open-source Deep Dive","Elixir","Broadway","data processing","message queue","message queues","concurrency","actor concurrency model","producer/consumer model","open-source","open-source project"],"title":"Open-source Deep Dive: Broadway (Part 1) - Message queues, concurrency in Elixir, and Broadway architecture"},{"categories":null,"contents":"This open-source deep dive has been split into two parts! The first part covers the prerequisite knowledge that would be good to know when trying to understand the inner workings of Broadway. The second part is an in-depth analysis of the implementation of various features of Broadway.\nThis is the second part of the deep dive and the following topics will be covered:\nRate limiting Batching messages Telemetry Creating a built-in testing support for pipelines Achieving graceful shutdowns Other interesting bits of code If you want a refresher on the concepts behind Broadway (like message queues and concurrency in Elixir) or to better understand Broadway\u0026rsquo;s pipeline architecture from a bird\u0026rsquo;s eye view, you can find the first part here!\nWhat\u0026rsquo;s the scoop? Now that we have explored the overall architecture of a Broadway pipeline, we can look at how certain features in Broadway are implemented.\nRate limiting Rate limiting refers to the act of limiting the amount of data that can be requested or processed in a given period of time\nRate limiting is applied across producers within a single pipeline to control the number of events emitted within a given period of time.\nThis is especially useful when the hardware of the machine running the pipeline is not able to keep up with processing large numbers of events demanded at a time — possibly due to a poorly configured pipeline.\nSome producers do not leverage the rate limiting feature of Broadway. For instance, the RabbitMQ producer creates an active listener, which means that event emission is not inhibited by the rate limiter. Instead, events are emitted the moment messages are published to the message queue ( unless otherwise configured) .\nBut for the producers that do leverage the rate limiting — such as the Amazon SQS producer — rate limiting is applied in two instances:\nWhen consumers make demands to the producer-consumer or producer\nIf the producer can still emit events, any demand made by the consumer will be handled by the producer. We take into account the rate limit threshold. If there are too many events to emit, the excess messages are stored in a message buffer that will have to be cleared later on.\nEach message that can be emitted will be transformed into the standard event structure that Broadway uses.\nIf the producer can no longer emit messages, any demand made is stored in a demand buffer that is cleared later on.\nWhen the rate limit is being reset after the given interval\nAfter the given interval, the rate limit threshold can be reset. However, we may have accumulated demands and messages in their respective buffers. We may find that the threshold has not been met before we reset it. Thus, we can use this remaining threshold to clear any lingering demands and messages stored in their respective buffers.\nOnce we have cleared as many messages as our remaining threshold allows, we will reset the threshold and schedule for another reset. These resets are scheduled at fixed intervals.\nThe rate limiting threshold is maintained as an atomic (discussed later on). This atomic array is generated by the RateLimiter process. This module handles all behavior surrounding working with the rate limit threshold. ProducerStage handles the actual logic of managing the demands of consumers.\nWhen the producer cannot emit any more events, i.e. the threshold has been reached, an internal state is set to :closed to avoid future demands from being handled.\nBatching Batching groups events based on given properties and sends them to designated \u0026ldquo;sub-pipelines\u0026rdquo; or batch processors to be handled. For instance, we might design a pipeline that stores events with even numbers in an S3 bucket and ones with odd numbers on Google Drive.\nThe Batcher process is assigned unique names for identification and events that are emitted from the producer must be tagged to a batcher. Failure to do so will result in a runtime error. This only applies if batching is enabled.\nIn order for the producer to send the appropriate events to the respective batcher, a PartitionDispatcher is used. Essentially, it defines the behavior of how events are emitted to consumers. A PartitionDispatcher dispatches events to certain consumers based on a given criteria (defined as a hash function). In this case, the criterion is the name of the batcher from the given event. This means that when we assign a batcher to the event, it will be dispatched to only that batcher. More information about dispatchers in GenStage can be found in the official documentation.\nEven within the batcher, further grouping can be made based on a batch key assigned to the event. This may be used to ensure that certain events are processed together. Internally, the batcher will accumulate events before emitting them. However, as it cannot sit around accumulating events forever, a batch is emitted at regular intervals regardless of how many events are stored in it.\nThe BatchProcessor process handles a single batch at a time. It is similar to a regular processor, except it works on a batch of events. The handle_batch callback is used here.\nTelemetry Telemetry is used in Broadway to benchmark certain operations that occur such as the duration that a handle_message callback takes.\nBroadway relies on the telemetry library. Within the code, events are emitted when these operations occur and key measurements such as duration are tracked. Handlers/listeners of these events can be setup to respond to these events.\nTelemetry is not an Elixir-only feature. It is commonly used to perform application monitoring. OpenTelemetry is a really interesting framework that offers powerful application monitoring through telemetry.\nBuilt-in testing To test the pipeline, we should focus on ensuring that the data processing aspect of the pipeline works as intended. However, as we rely on external services for input, it would be hard to coordinate a test suite to work with a live data source as we may not be able to replicate the data source or publish data to the data source at will due to access limitations. Thus, Broadway has designed a testing utility that allows us to test the pipeline\u0026rsquo;s data processing capacity without relying on the data source.\nBroadway provides a placeholder producer module. This producer does not rely on any data sources. Instead, messages are emitted directly into the pipeline.\nThe producer module should be tested separately if there is core behavior that cannot be tested along with the pipeline.\nThis form of unit testing ensures that we reduce potential points of failure in our test suite if any of the aforementioned problems with using the original data source should surface.\nGraceful shutdowns Broadway boasts about having graceful shutdowns. This is a rather interesting concept to explore as it relies heavily on the concurrency system of Elixir.\nEssentially, the pipeline can only exist in two states — when all components are online and when all components are shutting down. There is no point in time where a single component will shutdown on its own without being restarted. This is because of the way that the supervisor of each component declares restart strategies ensuring that should a child process encounters any errors, it will be restarted without a hitch. This way, the only time where our components can shut down is when we shut down our main process or pipeline supervisor process. When either process is terminated, we want to properly handle all remaining events in the pipeline before shutting off every component.\nThis is achieved through a mix of concurrency features. But before we can explain how it works, a simple introduction of exit signals and process termination is due.\nExit signals and process termination Processes can be linked to one another. When either process receives an exit signal — which can occur when the process is terminated forcibly or when it receives an exit signal propagated from its parent — it will propagate the exit signal to the linked process and that process will terminate as well.\nHowever, these exit signals can be trapped instead. When this occurs, rather than terminating the process that receives the propagated exit signal, the exit signal is sent as a message, allowing the receiving process to handle the exit as though it was just another message.\n1 2 3 def handle_info({:EXIT, from, reason}, state) do # ... end When a process is terminated, an optional terminate/2 callback can be declared to perform any cleanup before the process is actually terminated. This is useful if we have any lingering operations that should be completed before we terminate the process.\nSupervisors can start a list of child processes and is responsible for managing the restart strategy of each child. The interaction between a supervisor and terminate is rather interesting. When a child is terminated, it is restarted accordingly. When a supervisor terminates, all of its children will also be terminated. If a child process traps exits, the terminate callback is called. If not, it will simply terminate immediately without calling the callback. More information about how supervisor interact with shutdowns can be found in the official documentation.\nBack to our regularly scheduled deep dive\u0026hellip; With a basic understanding of exit trapping and process termination, we can actually understand how graceful shutdowns in Broadway works.\nWhen the main process or the pipeline supervisor process is terminated, the main process — which traps exit signals — will invoke its terminate callback which will inform the Terminator process to begin trapping exits and terminate our pipeline supervisor. As this Terminator process is a child of the pipeline supervisor, it will invoke its implementation of terminate.\n1 2 3 4 5 6 7 8 9 10 11 12 @impl true def terminate(reason, %{name: name, supervisor_pid: supervisor_pid, terminator: terminator}) do Broadway.Topology.Terminator.trap_exit(terminator) ref = Process.monitor(supervisor_pid) Process.exit(supervisor_pid, reason_to_signal(reason)) receive do {:DOWN, ^ref, _, _, _} -\u0026gt; :persistent_term.erase(name) end :ok end The exit signal propagates to the other components through their supervisors terminating and they will also invoke their terminate callback if they trap exits such as producers disconnecting from the data source.\nThe Terminator process is responsible for ensuring that all events still within the pipeline are processed before terminating the pipeline entirely.\nIt does so in three phases:\nNotify that the processors do not resubscribe to producers through a state flag Drain the producers of any events remaining by emitting the events through the pipeline Wait for the batch processors (which will be the very last component in the pipeline) to terminate before terminating the supervisor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @impl true def terminate(_, state) do for name \u0026lt;- state.first, pid = Process.whereis(name) do send(pid, :will_terminate) end for name \u0026lt;- state.producers, pid = Process.whereis(name) do Broadway.Topology.ProducerStage.drain(pid) end for name \u0026lt;- state.last, pid = Process.whereis(name) do ref = Process.monitor(pid) receive do {:done, ^pid} -\u0026gt; :ok {:DOWN, ^ref, _, _, _} -\u0026gt; :ok end end :ok end Interestingly, as the producer may be waiting to drain events, we may not want to cancel all of its consumers immediately. Thus, we rely on GenStage#async_info to queue the message to cancel all consumers at the end of the GenStage message queue — effectively waiting for all other events to be processed before cancelling all consumers. If batching is enabled, the processors will also wait for the batches to be processed before cancelling all consumers.\n1 2 3 4 5 6 7 @spec drain(GenServer.server()) :: :ok def drain(producer) do GenStage.demand(producer, :accumulate) GenStage.cast(producer, {__MODULE__, :prepare_for_draining}) # The :cancel_consumers message is added to the end of the message queue GenStage.async_info(producer, {__MODULE__, :cancel_consumers}) end These mechanisms ensure that all events left in the pipeline is properly processed before the pipeline terminates, thus achieving graceful shutdowns.\nFascinating discovery! These are some interesting bits of code that Broadway has.\n__using__ configurations Like other libraries in Elixir, use Broadway is where it all begins. As discussed in the previous open-source deep dive, the behavior of use can be altered by defining the __using__ macro.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 defmacro __using__(opts) do quote location: :keep, bind_quoted: [opts: opts, module: __CALLER__.module] do @behaviour Broadway @doc false def child_spec(arg) do default = %{ id: unquote(module), start: {__MODULE__, :start_link, [arg]}, shutdown: :infinity } Supervisor.child_spec(default, unquote(Macro.escape(opts))) end defoverridable child_spec: 1 end end There are three interesting bits of code in the __using__ macro:\nlocation: keep\nUsed to report runtime errors from inside the quote. Without this, errors are reported where the defined function ( in quote) is invoked. This is to ensure that we are aware of where the errors are occurring. More information about this configuration can be found here.\nbind_quoted\nUsed to create bindings within the quote. When a binding is created, the value is automatically unquoted (which includes evaluation) and the value cannot be unquoted again. This is especially used when we do not want to re-evaluate the value multiple times.\nMore information quoting and unquoting in Elixir can be found in the official tutorial and a simplified explanation and example of binding can be found here.\n@behaviour\nUsed to define interface-like behavior where modules that adopt these behaviors can implement callbacks defined. In this case, when a module use Broadway, it will have to implement certain callbacks like handle_message while other callbacks like handle_batch remain optional.\n1 2 3 4 5 6 7 8 9 10 11 12 @callback prepare_messages(messages :: [Message.t()], context :: term) :: [Message.t()] @callback handle_message(processor :: atom, message :: Message.t(), context :: term) :: Message.t() @callback handle_batch( batcher :: atom, messages :: [Message.t()], batch_info :: BatchInfo.t(), context :: term ) :: [Message.t()] @callback handle_failed(messages :: [Message.t()], context :: term) :: [Message.t()] @optional_callbacks prepare_messages: 2, handle_batch: 4, handle_failed: 2 More information about typespecs can be found in the official documentation .\nModule metadata processing While on the topic of meta-programming, module metadata can also be processed.\nensure_loaded? ensures that a given module is loaded. In Broadway, this is used to ensure that the :persistent_term module from Erlang is available for Elixir — the only time it will not be available is when the version of Elixir is too old. Documentation here.\n1 2 3 4 5 unless Code.ensure_loaded?(:persistent_term) do require Logger Logger.error(\u0026#34;Broadway requires Erlang/OTP 21.3+\u0026#34;) raise \u0026#34;Broadway requires Erlang/OTP 21.3+\u0026#34; end function_exported? returns whether a module contains a definition for a public function with a given arity. Used to execute functions from modules if they are defined. Documentation here.\n1 2 3 4 5 6 7 8 if Code.ensure_loaded?(producer_mod) and function_exported?(producer_mod, :prepare_for_start, 2) do case producer_mod.prepare_for_start(module, opts) do {child_specs, opts} when is_list(child_specs) -\u0026gt; {child_specs, NimbleOptions.validate!(opts, Broadway.Options.definition())} other -\u0026gt; # ... Dynamic process naming As the pipeline can comprise of any number of components, Broadway supports dynamically generated processes. These dynamically generated processes are assigned names that follow a fixed convention — comprising of the name of the pipeline, the process type, and the index of the component among the other components of the same type.\n1 2 3 4 5 6 7 8 9 defp process_name(prefix, type, index) do :\u0026#34;#{name_prefix(prefix)}.#{type}_#{index}\u0026#34; end defp process_names(prefix, type, config) do for index \u0026lt;- 0..(config[:concurrency] - 1) do process_name(prefix, type, index) end end The names are returned as quoted atoms where the atom has a space in it so it has to be declared via :\u0026quot;\u0026quot; .\nStorage options in Elixir Besides the basic data structures like lists and dictionaries, Elixir and Erlang offer other ways of storing data within processes.\nAtomics\n:atomics are a way of performing atomic operations on a set of mutable atomic variables.\nUsed to maintain the rate limiting threshold.\nPreviously, the rate limiter used ETS instead but atomic operations are much better for concurrent systems as they avoid race conditions when multiple producer processes are attempting to modify the rate limit.\n1 2 counter = :atomics.new(@atomics_index, []) :atomics.put(counter, @atomics_index, allowed) Persistent term\nStorage for Erlang terms that is optimised for reading terms at the expense of writing and updating terms.\nUsed to store pipeline metadata like producer names etc.\n1 2 3 4 5 6 7 8 :persistent_term.put(config.name, %{ context: config.context, producer_names: process_names(config.name, \u0026#34;Producer\u0026#34;, config.producer_config), batchers_names: Enum.map(config.batchers_config, \u0026amp;process_name(config.name, \u0026#34;Batcher\u0026#34;, elem(\u0026amp;1, 0))), rate_limiter_name: config.producer_config[:rate_limiting] \u0026amp;\u0026amp; RateLimiter.rate_limiter_name(opts[:name]) }) Queue\nManage first-in, first-out queues.\nUsed to manage message and demand buffers in the producer.\n1 2 3 4 # A queue of \u0026#34;batches\u0026#34; of messages that we buffered. message_buffer: :queue.new(), # A queue of demands (integers) that we buffered. demand_buffer: :queue.new() Process dictionaries\nStore state within a process directly although its usage is generally frowned upon .\nUsed to store batches in the batcher.\n1 2 3 4 5 6 7 8 9 10 11 defp init_or_get_batch(batch_key, state) do if batch = Process.get(batch_key) do batch else # ... end end defp put_batch(batch_key, {_, _, _} = batch) do Process.put(batch_key, batch) end A better alternative might have been to use an Agent or ETS instead.\nEdit! I clarified with the team about their decision to use process dictionaries over ETS, this was their response:\nThe correct solution here would be to simply use a map. But because this is very intensive code, we need a mutable option, and the process dictionary is the most efficient one. ETS would be slow as data has to be copied in and out of ETS. This is one of the very cases where using the pdict for performance is justified. :)\nSo, the reason why they decided to use a process dictionary over ETS is due to the performance requirement of batching! Very interesting!\nOptions validation Dashbit — the team behind Broadway — developed an options validation library called NimbleOptions that aims to be a small library for validating and documenting high-level options.\nA set of definitions for the available options are created first and these can be used to validate a keyword list — aka the options.\nIf the options are invalid, an error is returned, otherwise an :ok status along with the options are returned. The returned options have default values filled in.\nDefault values in dictionaries Broadway has an interesting way of fanning out default values for the options keyword list. In the options keyword list, a \u0026ldquo;parent\u0026rdquo; value for :partition_by, :hibernate_after, and :spawn_opt is provided.\n1 2 3 4 5 6 7 options = [ partition_by: ..., # these are parent values hibernate_after: ..., producer: [ hibernate_after: ... # this is a child value ] ] The parent value will be used for producers, processors, and batchers if no explicit child value is provided. Alternatively, we might want to fan out a parent value to only two of the three unset child values while maintaining the original child value of the third.\nThis is done by merging the child options into the parent options. Thus, if the child does not define a value for the option, the parent value is inherited.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 opts = opts |\u0026gt; carry_over_one(:producer, [:hibernate_after, :spawn_opt]) |\u0026gt; carry_over_many(:processors, [:partition_by, :hibernate_after, :spawn_opt]) |\u0026gt; carry_over_many(:batchers, [:partition_by, :hibernate_after, :spawn_opt]) defp carry_over_one(opts, key, keys) do update_in(opts[key], fn value -\u0026gt; Keyword.merge(Keyword.take(opts, keys), value) end) end defp carry_over_many(opts, key, keys) do update_in(opts[key], fn list -\u0026gt; defaults = Keyword.take(opts, keys) for {k, v} \u0026lt;- list, do: {k, Keyword.merge(defaults, v)} end) end Closing the curtains To conclude, Broadway is a powerful library for building data processing pipelines. These pipelines are built on top of the robust concurrency system that Elixir boasts.\nBroadway is a very versatile library and the documentation contains detailed guides about using it with various data sources. Check out the Github repository and documentation!\nIf you want to get a basic understanding of the underlying concepts of Broadway or better visualise the architecture of a pipeline in Broadway, check out the first part here!\nOpen-source Deep Dive is a series where I pick apart open-source projects to explain the underlying concepts that power these projects and share my findings about the project!\n","date":"Apr 12","permalink":"https://blog.woojiahao.com/post/odd-broadway-2/","tags":["Open-source Deep Dive","Elixir","Broadway","data processing","message queue","message queues","concurrency","actor concurrency model","producer/consumer model","open-source","open-source project"],"title":"Open-source Deep Dive: Broadway (Part 2) - Inner workings of Broadway"},{"categories":null,"contents":" What is Hound? For browser automation and writing integration tests in Elixir\nLet\u0026rsquo;s inspect this definition a little closer\u0026hellip;\nWhat is browser automation? Browser automation is effectively the process of using a proxy (like Selenium or Hound) to perform browser actions on behalf of the user (like the test case). Essentially, we are automating the usage of the browser.\nIt is often associated with illegal applications like sneaker-botting but much like torrenting, there are positive applications and we will be exploring one of them in this post - integration testing.\nWhat is integration testing? When building software, we first build individual components to support given functional requirements. These individual components can be tested using unit tests - which ensure that given a set of inputs, the component returns a ** predictable** set of outputs (predictable means that the functions tested are pure).\nHowever, while components may work well on their own, when combined with other components (to form larger components/whole systems), unexpected behavior may be exhibited. For instance, the input from component A is transformed before it is used as input to component B, thus, the combined components returns an unexpected result.\nHence, integration tests serve to bridge the gap between individual components testing and full system testing.\nWhen combined with browser automation, we can ensure that a website works end-to-end. We can ensure that the data validation on the front-end works as intended and that the forms submitted by users are properly sent to the back-end and saved in the database.\nApproaching browser automation integration testing\u0026hellip; We can take two approaches to browser automation integration testing. We could either\nbuild our own interfacing system to communicate with the browser, or rely on existing interfacing systems The former is time-consuming and requires a lot of care during development as we have to account for varying browser APIs and quirks. Thus, it is wiser to chose the latter when approaching browser automation integration testing. Doing so minimizes the number of components we have to manage.\nIntroducing Hound! This is where Hound comes into the picture. Hound provides a clean API to build browser automation tests. It relies on Selenium, PhantomJS, and ChromeDriver as the interfacing systems to perform the \u0026ldquo;dirty\u0026rdquo; work of coordinating requests/responses to/from the browser.\nThis introduces a larger question, what exactly is Selenium, PhantomJS, and ChromeDriver? More importantly, in fact,\n\u0026ldquo;How is browser automation performed?\u0026rdquo;\nUnderstanding how browser automation is performed provides us with a better foundation to grasp how these technologies work and how Hound works under the hood.\nThe world of web drivers\u0026hellip; The key driving (pun intended) of browser automation is web drivers. But before we can understand what they are, we should establish some basic understanding of what a driver is in general computing terms.\nWhat are drivers? Drivers are pieces of software that behave as a proxy between a caller and a target. Callers can be something like the print prompt in Google Chrome or a computer peripheral. Targets can be something like the printer or computer.\nIn general, drivers are responsible for translating the caller\u0026rsquo;s request into a given format that the target can understand.\nThere may be variations of a caller to the same target so each driver must be able to translate their respective caller\u0026rsquo;s request into a common request format for the target. For instance, there are multiple types of keyboards that can be connected to a single computer but the computer can only understand a single request format. So the respective keyboard drivers are responsible for converting the unique keyboard\u0026rsquo;s requests into the format that the computer accepts.\nBack to web drivers Similar to general drivers, web drivers behave as proxies for the caller (Hound) to communicate with the target ( browser). It allows the caller to send instructions for the browser to perform. In the web development world, a browser is also referred to as a user agent.\nThe Selenium project proposed a W3C specification to guide the development of web drivers. For the rest of this discussion, we will be relying on this specification. The specification can be found here.\nAccording to the specification, there must exist a separation of concern when designing a web driver. More specifically, there are two components to a web driver:\nLocal end - API for developers to send requests to the browser (libraries like Selenium and Hound) Remote end - responsible for communicating with the browser, i.e. a browser driver (can you infer what this means?) In essence, a web driver is comprised of an API and a browser driver. Ideally, the API should be able to work with different browser drivers for different browsers.\nThe remote end must also provide an HTTP compliant wire protocol where each endpoint maps to a command for the browser.\nThis means that the remote end relies on HTTP to communicate requests with the browser. The remote end is a HTTP server that the local end writes HTTP requests to. The remote end translates each HTTP request (based on endpoint and method) to a command for the browser. Note that a wire protocol is a method of getting data from one point to another. It dictates that requests should follow a given format.\nThe specification also provides an outline for the endpoints that the remote end must make available for the local end. This ensures standardization and ease of adoption for future browser drivers.\nOne advantage to using a HTTP server for the remote end is that it is possible to host the remote end on a remote machine. This means that we can delegate the job of integration testing to another machine, a process commonly known as distributed testing. By enabling distributed testing, the local machine is not burdened with the responsibility of testing potentially extensive and rigorous integration tests which the machine may not support.\nSo, how does Selenium work? Selenium implements the web driver specification (they did author it). The remote end uses the JSON Wire Protocol as the HTTP compliant wire protocol to communicate with the browser driver. Note that the documentation provided by Selenium (for the JSON Wire Protocol) has been obsoleted in favor of the one defined in the specification.\nHow does Selenium differ from PhantomJS? PhantomJS is a headless browser library. Headless browsers are essentially web browsers without the graphical interfaces. Selenium, on the other hand, is a web driver. The key difference between the two is the way requests are routed and managed (PhantomJS is a rather interesting project so Open-source Deep Dive: PhantomJS edition maybe?).\nHowever, Selenium supports headless browsers as well and more importantly, Selenium is still in active development while PhantomJS has been archived due to a lack of active contributions.\nWhat is ChromeDriver then? ChromeDriver is a browser driver developed as part of the Chromium project. It is used by Selenium as one of the supported browser drivers. However, Hound supports raw requests to ChromeDriver as the underlying HTTP server works the same with or without the use of Selenium. It is an interesting project so I may explore it in another installment of Open-source Deep Dive!\nHound: Under the hood Leveraging browser drivers With a better understanding of how browser automation and web drivers work, we can in fact see that Hound doesn\u0026rsquo;t rely on the entirety of Selenium (including the local end APIs). Instead, it relies on the remote end of the Selenium web driver (along with PhantomJS and ChromeDriver) to minimize the number of \u0026ldquo;moving components\u0026rdquo; that need to be managed while reaping the benefits of the existing technologies. Thus, it can focus on delivering a seamless API for developing browser automation integration tests.\nExploring a basic use case We will inspect a basic use case of Hound before diving into how Hound works.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 defmodule HoundTest do use ExUnit.Case use Hound.Helpers hound_session() test \u0026#34;the truth\u0026#34;, meta do navigate_to(\u0026#34;https://google.com\u0026#34;) element = find_element(:class, \u0026#34;search\u0026#34;) fill_field(element, \u0026#34;Apples\u0026#34;) submit_element(element) assert page_title() == \u0026#34;Apples\u0026#34; end end There is quite a bit to unpack here. Let\u0026rsquo;s first understand the core of how a test suite with Hound is setup.\nFirst, we use ExUnit.Case as Hound works hand in hand with ExUnit, a built-in Elixir library for developing unit tests. It relies on two components of ExUnit: setup and on_exit. This allows Hound to work as expected.\nThen, we use Hound.Helpers which, with the power of macros, imports all helper functions that are required to access the browser session.\nFinally, we call hound_session() which creates a new session (an instance of the browser) and initializes the setup and tear down functionality of a Hound browser automation test.\nOnce the core of the browser automation test is built, we can write test cases as per normal, leveraging on functions like navigate_to() and fill_field() to perform browser actions. The intended behavior of these functions are easy to understand and the documentation for them can be found here.\nBreaking it down With a basic understanding of how a test suite can be setup in Hound, we can start to decompose Hound to better understand what makes it tick.\nFirst, we need to inspect the following file: lib/hound/helpers.ex which houses the Hound.Helpers module, the same one that we use in the example above.\nBy overriding the __using__ macro, Hound is able to import all of the helper functions into a given file with a single use statement. This helps to minimize the boilerplate for users to get started. Macros are meta programming constructs that inject code during compile-time. More on macros here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 defmacro __using__([]) do quote do import Hound import Hound.Helpers.Cookie import Hound.Helpers.Dialog import Hound.Helpers.Element import Hound.Helpers.Navigation import Hound.Helpers.Orientation import Hound.Helpers.Page import Hound.Helpers.Screenshot import Hound.Helpers.SavePage import Hound.Helpers.ScriptExecution import Hound.Helpers.Session import Hound.Helpers.Window import Hound.Helpers.Log import Hound.Helpers.Mouse import Hound.Matchers import unquote(__MODULE__) end end Hound.Helpers also defines the hound_session() function which relies on setup and on_exit() of ExUnit to setup and tear down a session between every test case.\n1 2 3 4 5 6 7 8 9 10 11 defmacro hound_session(opts \\\\ []) do quote do setup do Hound.start_session(unquote(opts)) parent = self() on_exit(fn -\u0026gt; Hound.end_session(parent) end) :ok end end end Each helper function constructs a HTTP request to the browser driver server using Hackney . For instance, navigate_to - which opens a given URL in the session - creates the following HTTP request:\n1 2 3 4 5 def navigate_to(url, retries \\\\ 0) do final_url = generate_final_url(url) session_id = Hound.current_session_id make_req(:post, \u0026#34;session/#{session_id}/url\u0026#34;, %{url: final_url}, %{}, retries) end Hound rolls its own HTTP request/response management system that supports multiple retries. This can be found in the lib/hound/request_utils.ex file.\nWe have managed to break down the core functionality of Hound. There are additional interesting components to Hound that I would like to explore as well.\nProcesses Applications are started according to standard OTP specification (here). lib/hound.ex starts a link to Hound.Supervisor which initializes two workers: Hound.ConnectionServer and Hound.SessionServer. These are child processes (Hound isn\u0026rsquo;t fully up-to-date with Application convention) that the supervisor manages.\n1 2 3 4 5 6 7 8 def init([options]) do children = [ worker(Hound.ConnectionServer, [options]), worker(Hound.SessionServer, []) ] supervise(children, strategy: :one_for_one) end Let\u0026rsquo;s explore what the connection server and session server are all about next.\nMore information on processes in Elixir here.\nConnection server This process is responsible for managing the details of the browser driver and providing information to construct the HTTP server endpoints. It stores the driver information using Agent to allow the information to be accessed across processes.\nMore information on Agent here.\nSession management Sessions, as mentioned earlier, refer to instances of the browser that we want to run our tests on. As Hound supports multiple sessions across different processes, it has rolled a session management system.\nSession management in Hound relies on ETS, a built-in storage option provided by Erlang and available in Elixir. When the session server first starts, it creates a new ETS table to hold the session information. This server is setup as a GenServer which allows it to support asynchronous and synchronous callbacks from other processes.\n1 2 3 4 def init(state) do :ets.new(@name, [:set, :named_table, :protected, read_concurrency: true]) {:ok, state} end When a session is first created by hound_session(), the current process\u0026rsquo;s ID (by default, it\u0026rsquo;s the main process) is related to the session. The process is monitored and the new session is created. Under the process ID, multiple sessions can be created, thus allowing Hound to support multi-session testing. Each session is identified by an ID. Each session is also assigned a name. By default, we use the session name of :default. The ETS table holds the following information (mapped to JSON for illustration purposes. In reality, ETS tables store tuples of data so the actual data stored does not include any keys, just the values in the given order):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [ { // Process ID \u0026#34;pid\u0026#34;: ..., // Process monitoring ref \u0026#34;ref\u0026#34;: ..., \u0026#34;session_id\u0026#34;: ..., // Map containing all sessions running \u0026#34;session\u0026#34;: { \u0026#34;\u0026lt;session_name\u0026gt;\u0026#34;: \u0026#34;\u0026lt;session_id\u0026gt;\u0026#34;, ... } } ] As you can see, a session name can be assigned to the same session ID, but not the other way around (I am not too sure why this is setup as such, more investigation would be required).\nWith the ETS table setup to manage session information, we can avoid a major problem: passing around the session ID to various functions. If we had done so, we would have increased the overhead required when using the session ID as functions would have to be designed to accept the session ID and we would have to devise a method of passing the session ID around.\nInstead, the session ID is retrieved from the server on demand using the current_session_id() function in lib/hound.ex.\nAs the current session ID is related to the calling process ID, multiple processes can have different sessions, thus, providing multi-session testing support. This also means that if the calling process changes, the associated session will be retrieved or a new session will be created dynamically.\nIf a process dies - i.e. Process.monitor() sends a DOWN message - the session server will destroy all associated sessions with that process asynchronously.\n1 2 3 4 5 6 def handle_info({:DOWN, ref, _, _, _}, state) do if pid = state[ref] do destroy_sessions(pid) end {:noreply, state} end Configurations Configurations are managed using Elixir\u0026rsquo;s Config API which uses keyword parameter lists to manage configurations.\n1 2 3 4 import Config config :hound, browser: \u0026#34;firefox\u0026#34; The configurations are stored as application environment variables which are retrieved by the connection server.\n1 driver = options[:driver] || Application.get_env(:hound, :driver, \u0026#34;selenium\u0026#34;) More information on the Config API here.\nCoding conventions Other rather interesting bits of Elixir convention that Hound employs are:\n^ (pin) operator\nThe pin operator ensures that a variable, when matched during assignment, is the same as the existing variable of the given name\nIn Hound, this is used to ensure that the retrieved process ID of session (from the session server) is the same as the given process ID (from argument).\nIf the retrieved pid does not match the pid argument, an error is raised.\n1 2 3 4 5 6 def all_sessions_for_pid(pid) do case :ets.lookup(@name, pid) do [{^pid, _ref, _session_id, all_sessions}] -\u0026gt; all_sessions [] -\u0026gt; %{} end end More information on the pin operator here.\ndefdelegate\ndefdelegate dictates that a function\u0026rsquo;s underlying behavior is deferred to that of another function in another module.\nThis allows a module to house the functionality of different modules without breaking the modularity afforded by the module system.\nInterestingly, the __using__ override in Hound.Helpers can be replaced with a multitude of defdelegate to the helper functions but it would, understandably, create a lot of confusion.\nMore information on defdelegate here.\n= (match operator) in function parameters\nAs = is the match operator in Elixir, it can be used to perform pattern matching while assigning the matched pattern to a variable name.\nThis is very useful when working with structures as you may not want to deconstruct the entire structure while ensuring that arguments follow the given structure.\n1 2 3 def foo(%User{} = user) do IO.puts user[:name] end More information on the match operator here.\nPattern matching as enums\nPattern matching with atoms can be used as substitutes for typical enum behavior.\nAn enum in Kotlin may look like:\n1 2 3 4 5 6 7 enum class MatchClause(val name: String) { CLASS(\u0026#34;class\u0026#34;), CSS(\u0026#34;css selector\u0026#34;), NAME(\u0026#34;name\u0026#34;), ID(\u0026#34;id\u0026#34;), ELEM(\u0026#34;element\u0026#34;) } In Elixir, it can be written as such:\n1 2 3 4 5 def match(:class), do: \u0026#34;class\u0026#34; def match(:css), do: \u0026#34;css selector\u0026#34; def match(:name), do: \u0026#34;name\u0026#34; def match(:id), do: \u0026#34;id\u0026#34; def match(:elem), do: \u0026#34;element\u0026#34; Conclusion To conclude, Hound is a browser automation and integration testing library built on top of web driver - more specifically, browser driver - technologies as it leverages Selenium, PhantomJS, and ChromeDriver to build a highly abstracted and simple to use API for building integration tests.\nUnder the hood, Hound is an intriguing project that uses fundamental constructs to build powerful internal libraries that support complex operations.\nIf you are interested in the topics discussed in this post, here are some additional readings:\nWhat is a web driver in Selenium? W3C WebDriver Specification Hound GitHub repository Hackney GitHub repository Distributed testing What is PhantomJS? ChromeDriver repository Uses of browser automation BitTorrent protocol ExUnit Application conventions in Elixir Process monitoring ETS GenServer NOTE: I do not condone the use of browser automation or torrenting for illegal purposes. Any links or discussions about the mentioned subjects are purely for educational purposes and should remain as that.\nOpen-source Deep Dive is a series where I pick apart open-source projects to explain the underlying concepts that power these projects and share my findings about the project!\n","date":"Jan 17","permalink":"https://blog.woojiahao.com/post/odd-hound/","tags":["Open-source Deep Dive","Elixir","Hound","Browser automation testing","Selenium","PhantomJS"],"title":"Open-source Deep Dive: Hound"},{"categories":null,"contents":"It\u0026rsquo;s 3am. You are one commit away from launching your first release of your latest idea. It\u0026rsquo;s an AI system to help people recommend names for their pets through pictures (clearly, Spot isn\u0026rsquo;t a great name). You take a swig of coffee. You fix the bug that has been sitting on the top of your Trello board for days now. git add . \u0026amp;\u0026amp; git commit -am \u0026quot;Fix issue where AI takes over the world\u0026quot; and you are off. Time to tag this release. This AI will be a hit and pet owners all around the globe can rejoice knowing that the arduous task of naming their pets will finally be over (now only if you had an AI to recommend you variable names). You push the tag and head off to bed. Weeks go by and you continue working on your revolutionary project. You add a feature that allows you to name pet rocks too. School starts. You get swept up with a sea of assignments and presentations. Soon, your baby starts to collect dust as you scramble to finish your assignments. Eventually, school finishes and you heave a sigh of relief. \u0026ldquo;Finally, I can add object naming too!\u0026rdquo; you think to yourself. But when you open up the project, you realise that the spark, that allure of the project is gone. You stare blankly at the Trello board of \u0026ldquo;To-Do\u0026rdquo; features and begin to see this project as a chore. Those new features and promised rewrites now seem like a hassle. Those surges of dopamine you got every commit, now reduced to a mere flicker. But this can\u0026rsquo;t happen, right? After all, this was your baby! You had spent an entire semester break building it, raising it. It can\u0026rsquo;t all be for nothing! What will happen to the poor pet owners? You try convincing yourself that you can power through this lack of motivation. You think to yourself, \u0026ldquo;Maybe if I just started here, I would be able to find joy in working on this again\u0026rdquo;. But alas, no matter how hard you will yourself, you cannot find an ounce of motivation to fix that bug, to merge that pull request, to add that feature. You sigh and start questioning yourself. If you cannot finish a mere side project, are you even that worthy of being a developer? But eventually, you find another side project to occupy your time. One that makes your heart race every commit. And the cycle continues.\nWhat I had just shared is - what I believe - a common narrative many developers experience. In programming, we are often bombarded with the idea that side projects are a necessary staple of a developer - that one must have a Github page filled to the brim with open-source contributions and personal projects to be considered a good developer. But is it really necessary? Or are we - as a collective - creating unecessary barriers for ourselves. Is this mindset really good for us? Or is it slowly destroying our motivation and sucking our weekends dry?\nIn this post, I would like to explore this aspect of the industry and talk about my experiences working on side projects and share some of my personal opinions on it. Now note, I do not claim to be a programming genius nor do I assume that my word is gospel. I am merely sharing my observations and experiences as a developer who was mostly self-taught. More importantly, this post is my way of performing introspection on my past five years as a developer and how much I have grown since then.\nSeparating the truth from fiction As easy as it is to denounce the notion of side projects as a parasite to one\u0026rsquo;s sanity. There are many truths and value that can be derived from making side projets. So, I would first like to explain why I believe side projects are a useful tool for developers.\nIt is hard to deny that side projects can be incredibly fun to work on. When you find a project idea that you believe in, its siren\u0026rsquo;s call is hard to resist. You\u0026rsquo;ll find yourself spending hours on end on it. Your weekends can be engulfed by the pile of features you have planned for.\nWhen someone stars our repository. When Google Analytics reports a new user. When someone makes a pull request. When we get a new issue. It is an undeniable fact that seeing people use what we have built is incredibly rewarding and often times, that drives us to want to work on it more; we now have an audience of users that we cannot disappoint.\nOf course, many of us embark on side projects for the intrinsic rewards it presents to us. This fun is usually derived from the prospect of learning something new. Learning how a network protocol works. Learning how a language is used. Learning a new framework. These are all carrots-on-sticks for developers. Naturally curious, we all look for mental stimulation and seek the next idea we can get our fingers on. Side projects serve as a great way to explore different technologies and collaborate with others.\nSide projects also offer a way for us, as developers, to view how much progress we have made. Remember that simple chat application you made when you were 16? Now look at you creating web applications from scratch or learning a whole network protocol on your own. These side projects document your journey as a developer and it is refreshing to look back and see how much you have grown through these projects.\nA case against side projects However, while the benefits of side projects are bountiful, it is hard to deny the \u0026ldquo;downsides\u0026rdquo; of side projects. Now, I use the word downsides with heavy quotations as I do not believe that these are direct downsides of the idea of side projects itself. Rather, I believe that the downsides to side projects are derived from the value we have placed on them as an industry and how we - as driven individuals - have warped this perception of a side project.\nI believe that side projects are unnecessarily prized and emphasised. The naturally competitive environment in tech has caused this idea that side projects are a quintessential component of any developer\u0026rsquo;s arsenal to become overly emphasised.\nIt is due to this over-emphasis that many who embark on side projects often develop feelings of guilts when they are unable to see a project through. It has caused the idea of side projects to transform from one of learning and exploration to one of a \u0026ldquo;do-or-die\u0026rdquo; nature. Worse still, those who do not devote time to side projects may feel as though they are losing out against others and in turn develop an obsession with trying to \u0026ldquo;catch up\u0026rdquo; to them. This also devalues the efforts of those who try at the beginning. I believe that this boils down to the fundamental behavior of humans: competition. We will use any - and every - metric we can get our hands on to compare ourselves against one another. This is also why I do not agree with those who try to micro-optimise their daily lives - trying to squeeze every second that passes to achieve optimal productivity. I was one of those who were also obsessed with ensuring that my every waking moment was meticulously planned to ensure that my productivity remained at its peak. This was born out of fear that the rapid advancements of this field would eventually overwhelm me and leave me in the lurch. However, the reality is, as much as it is important to be driven, we must recognise that there is more to life than an eternal grind to be productive. It is important that we look at the time we are spending on side projects objectively and questioning whether we are neglecting other aspects of our life in pursuit of this \u0026ldquo;grind\u0026rdquo;.\nThis can lead to people developing an unhealthy obsession with creating side projects and working on that \u0026ldquo;hustle\u0026rdquo;. While many feel incredibly motivated at the beginning, those who fail to properly pace themselves will end up burnt out. This can lead to a vicious cycle where they push themselves too hard only to feel exhausted, give up, and feel guilty about giving up - spurring them on to try even harder. Burn out - in tech - is one of the worst things that could happen to a developer in my honest opinion. When one\u0026rsquo;s livelihood depends on their ability to produce good code under a time crunch, a general sense of demotivation can lead one to become careless; introducing bugs into their system. It also diminishes one\u0026rsquo;s interest in the field and in the worst case, can drive them away from the field entirely.\nThere are those who also pursue the idea of side projects with the incorrect mindset. Often when I speak to beginners, they often state that they feel this obligation to start a side project because they have noticed that others are doing so. Some start this side project with the goal to become the number one starred repository on Github. Some dive into one because they think that the only way to improve as a developer is to work on side projects. Some also get swept up with the common myth that \u0026ldquo;side projects == necessity to work in a company\u0026rdquo; and end up pursuing this belief.\nHowever, what many of these individuals fail to realise is that side projects are not the only measure of a developer\u0026rsquo;s worth. As alluring of an idea it is that a company will hire you if you have a portfolio the length of a PhD graduate, companies are often looking to hire those who demonstrate problem-solving abilities. So what if you have made 20 APIs before? If you fail to demonstrate the most fundamental skill a developer should possess, you won\u0026rsquo;t be able to think critically to solve new problems. While side projects can build problem-solving abilities, some may only venture within the range of their comfort zone to try new side projects. In some cases, this could mean never trying any \u0026ldquo;hard problems\u0026rdquo; as they do not believe that they possess the capacity to solve them on their own.\nIt is this over-emphasis on side projects that has placed so much unnecessary pressure on new developers. I have met many developers who have been wrapped in this worry that if they don\u0026rsquo;t start building a spectacular portfolio right when they start, they will end up becoming a terrible developer. It is sad to see that they place this burden on themsevles when learning to program should be a fun and enjoyable thing.\nSo what now? In summary, I believe that side projects are not inherently evil. It is the warped perception of them that our industry has created that has de-valued the true essence of them.\nI have been on both ends of the \u0026ldquo;side project cycle\u0026rdquo; before. I have been the naive developer who thought having hundreds of followers on Github or thousands of stars on my projects mattered and I have also been burnt out, swamped with school work, and dissatisfied with my perceived notion of productivity.\nAs with anything in life, the key is balance. Understanding that side projects are not - in any way - sole markers of a developer\u0026rsquo;s abilities is important. Having a large following on Github does not mean you are a terrific programmer and vice versa. Learning to appreciate the learning process, rather than working on something for the potential rewards, is an important aspect of life and moreso in this culture where developers often try to one-up one another with false measurements of success. In the end, what you work on should matter to you. You can build something to benefit others, I am not discouraging that, but you must bear in mind that at the end of the day, the one who benefits most from it should be you.\nI think we have to look at each project objectively. \u0026ldquo;Have I learnt something about myself, this technology, or programming in general?\u0026rdquo; If yes, then I believe that you have already benefited greatly from it. Whether or not the project is \u0026ldquo;failed\u0026rdquo; or \u0026ldquo;incomplete\u0026rdquo;, as long as you have learnt something along the way, I do not believe that you should belittle that effort.\nSo go out and build something that inspires you! Work on the project that gets you excited. Explore technologies that you are interested in. Don\u0026rsquo;t worry so much about the perceived value of the project you are embarking on. This is YOUR journey, so enjoy it to the fullest!\n","date":"May 26","permalink":"https://blog.woojiahao.com/post/software-projects-truth/","tags":["reflection","sharing"],"title":"The truth behind software projects"},{"categories":null,"contents":"Over my time in TPH, I have noticed that a common woe aspiring bot developers have is that they are unable to host their Discord bot online as they may not have access to a credit card.\nIntroducing Heroku! While the official Discord bots used in TPH - like HotBot - is hosted via paid platforms, there are free alternatives to deploying your bot online. This is where Heroku comes into the picture!\nHeroku is a cloud platform that lets companies build, deliver, monitor and scale apps — we\u0026rsquo;re the fastest way to go from idea to URL, bypassing all those infrastructure headaches.\nHeroku\u0026rsquo;s free tier does not require any credit card information and has sufficient uptime for your basic bot development needs and it is a great starting place to understand hosting.\nHow does Heroku work? Before diving into setting up a Discord bot on Heroku, it is best to explain how Heroku is used. Heroku relies on the Git version control system (VCS) to manage an application. This means that it integrates well with any existing projects that already use Git. Do not fret, even if your application does not use Git, the configuration and setup for Heroku is still simple.\nBy using Git, Heroku receives the project files directly and it is responsible for building the project. This is unlike other hosting platforms where you would often only supply the final executable - a .jar file in our case - to the hosting platform to run.\nIn order for Heroku to understand how it will build and deploy your application, you must provide a Procfile.\nThe Procfile is comprised of two key components - the dyno to run the application on and the commands to run your application.\nAccording to the Heroku documentation on dynos, dynos are containers that are used to run and scale all Heroku applications. Rather than worrying about configuring your build environment or OS, you can focus on building your applications and allowing Heroku to take over the build and deployment process. For all Discord bots, we will use a worker dyno.\nThe build commands we supply correspond to the build commands we use to run our bots locally.\nAs Heroku uses the project files to determine the type of tools we are using, we do not need to specify the instructions to create the executable. In our case, since we are using Maven, it can intelligently detect the pom.xml file and create the .jar accordingly. This leaves us with only the run commands to include in our Procfile .\nFinally, to tighten security, we will store all bot tokens in Heroku\u0026rsquo;s config vars. From a code perspective, these config vars are simply environment variables available to our applications. This allows us to load our bot token during runtime and prevent the bot token from being leaked.\nThus, we can define our deployment plan as such:\nInitialise the codebase as a Git repository Create a Heroku application for the bot Create a Procfile to supply instructions for Heroku to run the bot Store the bot token as a config var to be used by your bot What I have just presented is a general overview of Heroku as a hosting platform. I will be diving into the implementation in the following sections.\nGetting started For this article, I will be using a very simple Discord bot written in Kotlin. I have chosen to use JDA as the focus of this guide is to understand Heroku. The code repository can be found here.\nIf you wish to follow along, you can get the repository via\n1 2 $ git clone https://github.com/woojiahao/discord-heroku-deployment-demo ping-bot $ cd ping-bot/ Aside from that, basic understanding of the following is good to have to understand the technical details of this guide.\nGit - version control system that integrates with Heroku to enable easy deployments Maven - build tool for Kotlin to manage application dependencies In Kotlin/Java, we are looking to create a .jar file. This .jar file can be thought of like a .exe file. Essentially, it bundles the application and allows us to run our bot without having to fire up an IDE.\nTo create this .jar file, we will use Maven. For more information about using Maven to create .jar files, refer to this guide.\nWith the formalities out of the way, let\u0026rsquo;s get down to deploying our bot.\nInstalling Heroku You will have to install Heroku onto your machine to execute the following commands in the command line. You can find the installation instructions for Heroku here.\nTo ensure that you have installed Heroku successfully, run heroku --version. My version of Heroku is heroku/7.39.2 linux-x64 node-v13.12.0\nSetup a Git repository As mentioned earlier, we need to ensure that our application is a Git repository for Heroku to work.\nWhile it is recommended to publish your repository to GitHub ( or any other version control website), it is not necessary for deploying your applicaiton to Heroku.\nIf you are using the sample bot, it is already a Git repository.\nIf you are deploying your own bot, initialise a repository by using the following command inside the root folder of your codebase.\n1 $ git init Create a new Heroku application Then, we want to create a Heroku application.\n1 $ heroku create [project name] The project name is optional and will be automatically generated if not provided. It is recommended that you give a name to be organised.\nTo ensure that the Heroku application has been created, run the git remote -v command to list the remotes of your repository. Should your application have been created successfully, you will see a new remote added linking to a Heroku Git remote.\nWith the Heroku application created, we can begin configuring our repository to deploy to Heroku.\nCreating a Procfile As explained earlier, the Procfile acts as a build instruction manual for our application. It instructs Heroku how we want to run our application. Heroku takes over the rest and helps with managing our build environment.\nFor my sample bot, the Procfile looks like this:\nworker: java -jar target/Bot.jar Let\u0026rsquo;s breakdown this file. We first declare the dyno type as worker. Then, we specify the command to run our .jar file.\nHeroku is able to intelligently detect that our Kotlin application uses Maven as a build tool and runs the mvn clean install command to create our Bot.jar file. Then, it will use the commands in the Procfile to run the application.\nSecuring Discord bot tokens A Discord bot requires a token to run.\nYou can obtain this bot token when you make a new Discord bot from the Discord developer dashboard. However, you do not want to expose this token in your repository as this would mean that others could launch and access your bot.\nAs mentioned earlier, we will make use of Heroku\u0026rsquo;s config vars to safely store and access this token.\nWe will add our bot\u0026rsquo;s token as an environment variable and use System.getenv() method to retrieve this value.\n1 $ heroku config:set BOT_TOKEN=\u0026lt;bot token\u0026gt; Inside the Bot.kt file, you will find the following lines in the main() function.\n1 2 val token = System.getenv(\u0026#34;BOT_TOKEN\u0026#34;) ?: throw Exception(\u0026#34;Must include bot token in environment variable for bot to run\u0026#34;) This will retrieve the corresponding environment variable that we have stored in Heroku. If there is no environment variable present, we will stop the bot from launching and display an error.\nAn additional benefit of storing our bot tokens as an environment variable is that we are able to store the bot token locally as an environment variable which streamlines our development process as we could have a separate token used for a development/testing bot.\nLaunching the bot After configuring everything, commit all the changes to your project, and push it to the heroku remote.\n1 2 3 $ git add . $ git commit -am \u0026#34;Setup Heroku\u0026#34; $ git push heroku master If you encounter a problem with pushing to the heroku remote, use the command heroku logs --tail and find the latest error messages to debug any errors.\nAfter pushing the changes, Heroku will build your application. However, it is not online yet as you have to scale your application. This tells Heroku how many instances of your application you wish to run. For our case, we can go with one worker dyno.\n1 $ heroku ps:scale worker=1 You can now invite your bot to a server and test it out. If you\u0026rsquo;re using the sample PingBot, you can use !ping and expect the bot to respond with Pong!.\nNow what? Congratulations! You have just deployed a Discord bot onto Heroku! When you make changes to the bot, you are free to commit and push those changes to the heroku remote to update the bot that is online.\nHere are some tips for developing with Heroku.\nWhile working on your development copy of the bot, it is recommended that you obtain a seprate bot token and attach it as an environment variable to your local development environment. Doing so allows you to maintain your bot\u0026rsquo;s uptime while making changes. If you encounter any errors or your bot is not responding, use the heroku logs --tail command to view the logs of your application. Doing so allows you to check if there were any errors while running your project. If you require persistent storage, Heroku comes with a free tier plugin for PostgreSQL. Heroku - by default - has ephemeral storage, meaning it does not maintain new files after each build. Conclusion Heroku offers a free alternative to many hosting platforms and is a perfect platform for aspiring bot developers to begin.\nMore resources on hosting JVM-based applications on Heroku:\nGetting Started on Heroku with Java Java Sample (on GitHub) ","date":"Apr 21","permalink":"https://blog.woojiahao.com/post/deploying-discord-bot-kotlin/","tags":["Kotlin","Heroku","Discord bot","deployment","tutorial","guide"],"title":"Deploying discord bots written in Kotlin to Heroku"},{"categories":null,"contents":"After a long hiatus induced by the demons that was school, I have finally defeated the final boss and graduated!\nWith that, I have started to work on re-building my portfolio site (you\u0026rsquo;re seeing it right now!). I wanted to learn Gatsby.js and so I figured it would be the best time to do so! While working with Gatsby.js, one of the issues that I had faced was not having a proper guide on deploying Gatsby.js applications onto GitHub Pages via GitHub Actions. The official documentation does include a guide on perform Continuous Deployment (CD) via Travis.CI but I wanted to mess around with GitHub Actions - GitHub\u0026rsquo;s own CI/CD pipeline.\nSo I decided to experiment with building my own workflow and document my process. It was quite interesting how I got to the current refinement of my CD workflow and I will be sharing a little on the mistakes that I had made and the lessons learnt.\nWhy Gatsby.js? Taken directly from Gatsby\u0026rsquo;s web description\u0026hellip;\nGatsby.js is a PWA (Progressive Web App) generator.\nIt may seem a little odd at first but basically, Gatsby.js is a static site generator built around React. It ties together various React plugins - like react-router and webpack - to create a seamless development experience when creating static sites.\nA static site is a site where a framework is used but the library generates the resulting HTML/CSS/JS files. In doing so, the site is far more performant than dynamic sites that take time to execute the code of the framework. Gatsby.js performs all the execution when creating a bundle and we simply deploy this bundle to GitHub pages or any web hosting platform to have our site up and running.\nIn a future post, I will be covering the merits of a static site generator - specifically Gatsby.js - but for now, let\u0026rsquo;s move on to our next tool, GitHub Pages!\nWhy GitHub Pages? GitHub pages is\u0026hellip;\nGitHub Pages is a static site hosting service that takes HTML, CSS, and JavaScript files straight from a repository on GitHub, optionally runs the files through a build process, and publishes a website.\n(Definition taken from here)\nI would like to focus on these four hyper-critical words: \u0026ldquo;static site hosting service\u0026rdquo;. Recall when I mentioned that Gatsby.js is a static site generator? These four words are like music to my ears! It means that once we have generated that bundle with Gatsby.js, we can use GitHub Pages to host our website!\nThis is awesome because I needed a cheap (if not nothing) hosting provider to host my portfolio website. GitHub Pages will deploy the site directly from a GitHub repository, providing seamless integration and it is super userful for creating documentation sites for your projects!\nA big perk to using GitHub Pages is that you can have a user page. This is created from a repository that follows this naming pattern: \u0026lt;GitHub username\u0026gt;.github.io. When using a user page, you will receive access to the .github.io domain. This is useful for portfolio sites as it now means that we are able to deploy our portfolio page and have a domain name like woojiahao.github.io!\nYou can find out more about GitHub pages here and more about user pages here.\nWhy GitHub Actions? Finally, we have the star of the evening - GitHub Actions!\nGitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub.\n(Definition taken from here)\nGitHub Actions allows us to develop CI/CD workflows that integrates directly with GitHub. For our use case, we can use the default tier.\nI picked GitHub Actions primarily because I was intrigued by it and wanted to give it a spin. I had adopted it when working on torrent.go (a BitTorrent protocol implementation written with Go!) and found that it was rather unique in its approach so I wanted to test it out even more.\nChief! What is our plan of attack? First, I would like to discuss the final strategy that I took to deploy my portfolio site to GitHub pages.\nTo begin, we need to outline the strict limitation of user sites\u0026hellip;\nIf the repository for your user or organization site has a master branch, your site will publish automatically from that branch. You cannot choose a different publishing source for user or organization sites.\nThis means that for our site, if we are using woojiahao.github.io, we MUST publish the site to the master branch.\nOur CD workflow looks a little like this:\nWe push our latest changes in Gatsby.js (including new blog posts, project listings, or just site changes) to a develop branch\nAs we may have multiple sets of changes that we push at different times, we do not want our site to be deployed immediately. Instead, we move on to step dos!\nWe will merge the changes into a publish branch. On merge, we should automatically deploy our application.\nNow this is where it gets tricky, we need to build our Gatsby.js page - to create the bundle - and set the contents of this bundle to the contents of the master branch.\nThis is where you can see our consideration of the limitation mentioned above take effect. However, by setting the master branch to be a \u0026ldquo;dump\u0026rdquo; for the bundle files, it would be hard for us to navigate our GitHub repository. This is why we need to properly setup our branches to allow for this peculiar workflow!\nGo, go, go! Let\u0026rsquo;s dive right into the configurations.\nSetting up branching The first order of business is to properly setup our branching strategy. We will use a mix of both the GitHub UI and the Git CLI. So if you have not installed the Git CLI, you can do so here.\nAs mentioned in our workflow, we will be using three branches:\nmaster - to hold the build bundle and renders the page develop - pseudo-master branch that we will use to host our changes in Gatsby.js publish - merge-only branch that we use to trigger a deployment So, let\u0026rsquo;s first create and push the other two branches.\n1 2 3 4 $ git checkout -b develop $ git checkout -b publish $ git push -u origin develop $ git push origin publish So we checkout the develop branch before the publish branch since we want the publish branch to be based on the latest changes of the develop branch. Don\u0026rsquo;t worry, however, we will only be needing to checkout the publish branch via the Git CLI only once to configure.\nWe push and set the develop branch to be our tracking branch as from now on, we want to permanently push to the remote develop branch.\nWe also push the publish branch to create the remote branch.\nIf you go to the repository in GitHub and expand the \u0026ldquo;Branch\u0026rdquo; dropdown, you should see that there will be three branches now.\nOnce done, we can now continue our branching configurations in the GitHub UI.\nUnder \u0026ldquo;Settings \u0026gt; Branches\u0026rdquo;, there is an option for the \u0026ldquo;Default branch\u0026rdquo;. Normally, it is the master branch. However, as mentioned earlier, we want to be able to view and access our repository easily (and have the all-so-important language colors be right!). So, we will change the default branch to be develop.\nOnce we have configured these two aspects, we are done with our branching strategy! Let\u0026rsquo;s move on to the real meat of this CD - the workflow!\nCreating the workflow GitHub Actions will look out for workflows in our repository to determine what actions are to be run. These workflows are stored in the .github/workflows folder.\nWe can just name our CD workflow as deploy.yml. Yes, these workflow files are declared as YAML documents - like Docker Compose!\n/ |_.github/ |_workflows/ |_deploy.yml Then, we can start to configure our workflow. Fire up your favourite text editor and let\u0026rsquo;s begin!\nThe first thing we will configure is when this workflow will run. As mentioned earlier, we want to deploy our site only when we push to the publish branch via a merge, thus, we will have the following:\n1 2 3 4 on: push: branches: - publish Then, we can start declaring our jobs aka the thing that runs. GitHub Actions allows us to configure concurrent jobs or jobs that rely on one another, but I will not be going into detail about those in this post. If you want more information, please refer to the documentation here.\nThis workflow will only require one job and we can call it deploy.\n1 2 jobs: deploy: We then specify what is the underlying OS the CD will use to execute this workflow.\n1 runs-on: ubuntu-latest Then we declare strategies. From my understanding, these are variables that we can use and we can declare them as arrays to allow us to execute the workflow on multiple versions of Node for instance.\nHowever, in our case, we can make do with just having one version of Node since we are building once. If you require more details about strategies in GitHub Actions, refer here.\n1 2 3 strategy: matrix: node_version: [ 13.x ] Finally, we can declare our steps for the workflow.\n1 steps: First, checkout the repository so that our workflow can access it.\n1 2 - name: ... uses: actions/checkout@v1 Then, we setup Node in out Ubuntu machine. We use the node_version we declared in our strategy.\n1 2 3 4 - name: ... uses: actions/setup-node@v1 with: node-version: ${{ matrix.node_version }} Then, we install gatsby-cli and the necessary dependencies for our static site to be generated.\n1 2 - name: ... run: npm install -g gatsby-cli \u0026amp;\u0026amp; npm install --no-optional Then, we set the Git credentials as we are using the gh-pages npm package to deploy our site. I will explain further later on.\n1 2 - name: ... run: git config --global user.email \u0026#34;\u0026lt;insert GitHub email\u0026#34; \u0026amp;\u0026amp; git config --global user.name \u0026#34;\u0026lt;insert desired name\u0026gt;\u0026#34; Lastly, we can build the site bundle and deploy it. I have used a custom script declared in the package.json and a GitHub token, both of which I will be discussing below.\n1 2 3 4 - name: ... run: npm run deploy env: GH_TOKEN: ${{ secrets.GH_TOKEN }} There it is. We have created the workflow for deploying our application. However, we have some missing components that we have yet to configure so buckle up for our last set of configurations.\nA sample of the whole workflow file looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 on: push: branches: - publish jobs: deploy: name: Deploy website to Github Pages with Gatsby runs-on: ubuntu-latest strategy: matrix: node_version: [ 13.x ] steps: - name: Checkout uses: actions/checkout@v1 - name: Setup Node.js version ${{ matrix.node_version }} uses: actions/setup-node@v1 with: node-version: ${{ matrix.node_version }} - name: Installing Gatsby CLI run: npm install -g gatsby-cli \u0026amp;\u0026amp; npm install --no-optional - name: Setting Git credentials run: git config --global user.email \u0026#34;woojiahao1234@gmail.com\u0026#34; \u0026amp;\u0026amp; git config --global user.name \u0026#34;woojiahao\u0026#34; - name: Deploy the site run: npm run deploy env: GH_TOKEN: ${{ secrets.GH_TOKEN }} Creating a deploy script We are using the gh-pages to automatically push the build bundle to the master branch. We will need to setup a deploy script in our package.json to use this package. This script will build the bundle and deploy it to the master branch.\n1 2 3 4 \u0026#34;scripts\u0026#34;: { ..., \u0026#34;deploy\u0026#34;: \u0026#34;gatsby build \u0026amp;\u0026amp; gh-pages -d public -b master -r https://$GH_TOKEN@github.com/woojiahao/woojiahao.github.io.git\u0026#34; }, I would like to focus in on the command for gh-pages.\ngh-pages -d public -b master -r ... -d indicates which folder the build bundle is located. In Gatsby.js case, it is the public/ folder.\n-b indicates which branch we will push the build bundle to. By default, it is the gh-pages branch. But as I have explained, we need it to be the master branch.\n-r indicates the GitHub repsitory we will be pushing too. This uses the GH_TOKEN environment variable that we have loaded into the workflow. It also leads us nicely to the next and final configuration we need to perform - getting a GitHub access token.\nShhh! \u0026hellip; (Access tokens and secrets) The last ingredient to this workflow recipe is a GitHub access token.\nWe can generate a personal access token under \u0026ldquo;Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new token\u0026rdquo;.\nWhen generating the token, restrict the token\u0026rsquo;s scope to repo only.\nYou will receive a token once you create it. Copy this token to your clipboard and save it somewhere secure.\nThen, go to the Gatsby.js repository and under \u0026ldquo;Settings \u0026gt; Secrets\u0026rdquo;, select \u0026ldquo;Add a new secret\u0026rdquo;. The name of the secret must be GH_TOKEN and the value is the copied token.\nFor more information about personal access tokens and GitHub secrets in GitHub Actions, you can check these links out: access tokens and secrets.\nAnd we are done! This was all the configuration we needed to get the CD workflow working!\nIT WORKS!! We can test whether the workflow works by making a merge request from develop to publish. When we accept the merge request, it will push the changes to the publish branch and in turn, trigger the workflow and deploy our site.\nNow, just sit back and enjoy as the deployment happens automatically.\nMistakes were made\u0026hellip; While experimenting with GitHub Actions, I had several \u0026ldquo;duh\u0026rdquo; moments.\nYAML disaster When I was writing the first iteration of the workflow file, I failed to read the documentation for GitHub secrets and had incorrectly set the environment variable.\nFYI, environment variables should be declared like this\n1 2 env: - \u0026lt;key\u0026gt;: \u0026lt;value\u0026gt; and not like this\n1 env: ${{ secrets.GH_TOKEN }} Because I had declared it improperly, I was not able to access the GH_TOKEN token in my workflow and it caused the workflow to fail. It took me a while to realise what I had done.\nRepository takeover! When I was first designing my workflow, I had actually completely forgotten that there were branches outside of developer and master. Originally, I had created a completely separate repository to house my development code. This left the original repository to be a hosting platform for the build bundle.\nIt took me a while to realise how impractical and unnecessary this was and I re-designed the workflow.\nConclusion Overall, this was an interesting journey! I learnt a fair bit about Gatsby.js and GitHub Actions along the way. If you wish to view the Gatsby.js project I use for my portfolio site, you can visit it here\n","date":"Apr 14","permalink":"https://blog.woojiahao.com/post/gatsby-gh-pages/","tags":["tutorial","guide","Gatsby.js","GitHub Actions","GitHub Pages","continuous deployment","Javscript","GitHub"],"title":"Gatsby.js + GitHub Actions + GitHub Pages = Match Made in Heaven?"},{"categories":null,"contents":"You may be wondering what\u0026rsquo;s up with the over the top, nearly fan-ship name, trust me, I was not on anything when I wrote this.\nI simply wanted to write about a playground project I had written to get Docker working with Heroku, specifically in the context of writing a Discord bot in Kotlin.\nI made a rather comprehensive guide to go with the project over on GitHub and I wish to share it here as well since it was a rather large milestone.\nIf you wish to follow along, the sample project is found here.\nThe project uses Gradle as the build tool for the project.\nRunning the Docker image Local If you\u0026rsquo;re running the bot locally, you should use Docker to package the bot and run the Docker image created. This way, you can test your bot locally with the same environment as your server thanks to Docker!\n1 2 3 4 $ docker build -t discord-docker . $ docker image ls # Should see the image named \u0026#34;discord-docker\u0026#34; $ docker run -e BOT_TOKEN=\u0026lt;bot token\u0026gt; -d discord-docker $ docker container ls # Should see \u0026#34;discord-docker running\u0026#34; With Docker, it is as simple as that to get the bot running.\nHeroku If you\u0026rsquo;re deploying the bot on Heroku, the steps are actually outlined in the documentation for Docker by Heroku. (here)\n1 2 $ heroku stack:set container $ git push heroku master And watch as Heroku does its magic!\nProject composition build.gradle The build.gradle straightforward, with the use of the shadowJar plugin to create the fat jar required for all library dependency. In order to prevent the exported jar from having differing names, we set the archiveName attribute of the plugin to always use the name bot.${extension}.\nThis means that even if we changed the version of the gradle file, the exported jar file is the same name so we don\u0026rsquo;t need to modify our Dockerfile.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 plugins { id \u0026#39;org.jetbrains.kotlin.jvm\u0026#39; version \u0026#39;1.3.41\u0026#39; id \u0026#39;com.github.johnrengelman.shadow\u0026#39; version \u0026#39;5.0.0\u0026#39; } sourceCompatibility = 1.8 targetCompatibility = 1.8 group \u0026#39;com.github.woojiahao\u0026#39; version \u0026#39;1.0-SNAPSHOT\u0026#39; repositories { mavenCentral() maven { url \u0026#39;https://jitpack.io\u0026#39; } jcenter() } dependencies {\u0026#39;\u0026#39; implementation \u0026#34;org.jetbrains.kotlin:kotlin-stdlib-jdk8\u0026#34; implementation \u0026#34;com.gitlab.aberrantfox:Kutils:0.9.17\u0026#34; } compileKotlin { kotlinOptions.jvmTarget = \u0026#34;1.8\u0026#34; } compileTestKotlin { kotlinOptions.jvmTarget = \u0026#34;1.8\u0026#34; } sourceSets { main.java.srcDirs += \u0026#39;src/main/kotlin/\u0026#39; test.java.srcDirs += \u0026#39;src/test/kotlin/\u0026#39; } jar { manifest { attributes \u0026#34;Main-Class\u0026#34;: \u0026#34;BotKt\u0026#34; } from { configurations.compile.collect { zipTree(it) } } } shadowJar { archiveName(\u0026#34;bot.${extension}\u0026#34;) } Dockerfile The Dockerfile is a little more interesting as it makes use of multi-stage builds to create a minimal Docker image.\nOur first image layer uses the official gradle images. We will label this layer as builder. In this layer, our goal is to create the jar file that will contain all our dependencies. We first access the image as the root user and start with our working directory labelled as /builder. We then add all of our files into the working directory and finally, we construct the fat jar using the gradle shadowJar command.\nThen, we create another layer which will be the final layer that goes into the image. We first use the official Alpine linux image for OpenJDK 8. Then we create a working directory for our application labelled /app. Then we copy our fat jar from the builder layer to the our home directory. As soon as we are done, we then run the command to execute our fat jar and it will cause the Discord bot to launch.\nUsing Discord allows us to remain Gradle and Java version agnostic. This Dockerfile was taken and modified from the following article found here.\n1 2 3 4 5 6 7 8 9 10 FROM gradle:5.6.1-jdk8 AS builder USER root WORKDIR /builder ADD . /builder RUN gradle shadowJar FROM openjdk:8-jre-alpine WORKDIR /app COPY --from=builder /builder/build/libs/bot.jar . CMD [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;bot.jar\u0026#34;] heroku.yml The heroku.yml file contains the configuration needed for Heroku to run your application. In that sense, it is similar to the traditional Procfile that is provided to Heroku applications.\nIn this scenario, we don\u0026rsquo;t need to use an elaborate heroku.yml file, all we need is to specify that the worker dyno will be based off the instructions of the Dockerfile and that\u0026rsquo;s all.\nIf you do need to include information like addons and build steps, you can feel free to do so through the use of the additional properties within the configuration file. More information can be found here.\n1 2 3 build: docker: worker: Dockerfile ","date":"Aug 31","permalink":"https://blog.woojiahao.com/post/heroku-kotlin-discord-bot/","tags":["tutorial","guide","Kotlin","Heroku","Discord bot","Docker","deployment"],"title":"Heroku x Docker x Discord bot x Kotlin"},{"categories":null,"contents":" Announcement time! After a long hiatus from kMD2PDF, I revved up my engines and began to work on my latest planned feature for the project\u0026hellip;\nYAML support!\nYes, now YAML is in the works and you can now control basic attributes of your exported document using only front matter YAML.\nThis is an incredibly big milestone as this allows anyone to quickly customise their document without having to write a single line of Kotlin, and with another planned release to create a GUI exporter for markdown documents, this would greatly streamline users\u0026rsquo; experience.\nOkay, so what are delegate properties? kMD2PDF introduces a theming engine where you are able to change the entire color scheme of an exported document just by specifying an attribute in code as such\n1 2 3 4 5 6 7 val converter = markdownConverter { // ... settings { theme = Settings.Theme.DARK } } Design considerations Before building this system, we have to make several consideration\nHow do we specify that certain attributes have different configurations depending on the theme? How do we allow changes to the theme propagate to each element such that the changes are reflected during the document creation? Using singletons The second inquiry is rather easy to answer, so we will tackle that issue first - we can store the Settings as singleton, since the state of a singleton is unique and static, as long as the settings are configured before the document is created, then the settings will be chosen during runtime and have the exported document reflect the changes.\n1 2 3 4 5 6 7 object Settings { var font = FontFamily(\u0026#34;sans-serif\u0026#34;) get() = field.clone() // ... } inline fun settings(configuration: Settings.() -\u0026gt; Unit) = Settings.apply(configuration) Kotlin comes with a language construct to create singletons easily - object keyword (read more here).\nThis creates everything we need in a singleton but reduces all the boiler plate that would be involved with creating the singleton, unlike other languages like Java.\nWhen we update the Settings singleton, the changes will reflect with the exported document since the document style is lazily generated only up till the latest minute before it gets exported.\nMulti-value properties Now to tackle the hard question, how do we store multiple values in Kotlin? Ideally, what we would want with this system is to have a single variable and have that variable store both the dark theme and light theme setting. That is, if we had a single variable textColor, we would want to be able to store both the light theme and dark theme settings inside this variable. Depending on the current Settings.theme configuration at the time where the stylesheet has to be generated, the textColor variable would return either the configuration for DARK theme or LIGHT theme.\nTraditionally, we would approach this issue using a class to store the information and have that be the end of things - and this was indeed the approach I ended up employing due to certain limitations in that design which I\u0026rsquo;ll discuss.\nHowever, in Kotlin, there exists a language construct called delegated properties where you are able to call an object constructor to initialise a variable and provide it with a base of data. Subsequent times accessing this variable masks the object constructor and will only allow you to access the data type specified by the delegated property. You can think of a delegated object as an object that defaults all variable references to the given getValue and setValue attributes. This effectively means that once you\u0026rsquo;ve delegated a property, you are no longer entitled to modifying the object that created the delegate.\n1 2 3 4 5 6 7 8 9 10 import kotlin.reflect.* class DelegateExample(private var internalValue = \u0026#34;\u0026#34;) { operator fun getValue(thisRef: Any?, property: KProperty\u0026lt;*\u0026gt;) = \u0026#34;Internal value is ${internalValue}\u0026#34; operator fun setValue(thisRef: Any?, property: KProperty\u0026lt;*\u0026gt;, value: Int) { internalValue += value.toString() } } In the example above, we have created a delegated property that when referenced as a variable (get), will return a string. But when called with an assignment operator (set) will only accept Int.\n1 2 3 4 5 6 7 var value by DelegateExample() // internalValue = \u0026#34;\u0026#34; println(value) // Internal value is value = 10 // internalValue = \u0026#34;10\u0026#34; println(value) // Internal value is 10 value = 5 // internalValue = \u0026#34;105\u0026#34; println(value) // Internal value is 105 As you can see from, in order to instantiate a delegated property and use the features of getValue and setValue, we have to set the variable with the by keyword. This way, when you have variable references, it will always call the respective setValue/getValue methods.\nNow that you\u0026rsquo;ve had a crash course with delegated properties in Kotlin, can you think of how we can apply this concept to our current problem? To recap, we already have established a singleton to manage the configuration across all elements - and the configuration we are most interested in is the theme attribute which indicates which theme the document will create. In that case, since the state is singular and shared across the entire program, we can create a delegate property to hold the configuration of all the themes available, and when the time comes to generate the CSS for the document, the delegated property is called and it will return the corresponding attribute for the specific theme configured during the time of generation (since the document HTML/CSS are generated per call of MarkdownConverter#convert as opposed to generating them all at once).\nIn that case, we can create the delegated property as such:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class CssProperty \u0026lt;T\u0026gt;( private var light: T?, private var dark: T? = light, private var fallback: T? = null ) { operator fun getValue(thisRef: Any?, property: KProperty\u0026lt;*\u0026gt;) = when(Settings.theme) { LIGHT -\u0026gt; light DARK -\u0026gt; dark } ?: fallback operator fun setValue(thisRef: Any?, property: KProperty\u0026lt;*\u0026gt;, value: T?) = when(Settings.theme) { LIGHT -\u0026gt; { light = value } DARK -\u0026gt; { dark = value } } } This way, we can create a CssProperty to hold the configurations required for each theme (light/dark for now) and then when the variables are accessed, they will return the corresponding value stored depending on the current theme configured in the settings. The fallback property is a \u0026ldquo;default\u0026rdquo; for each CssProperty in the event where the light or dark property are both set to null.\nTo use the CssProperty class, we will use the following syntax:\n1 2 3 4 var textColor by CssProperty\u0026lt;FontFamily?\u0026gt;(c(\u0026#34;00\u0026#34;), c(\u0026#34;fa\u0026#34;)) print(textColor) // Light theme text color -\u0026gt; #000000 Settings.theme = Settings.Theme.DARK print(textColor) // Dark theme text color -\u0026gt; #fafafa And this provides us with such a convenient interface to modify the values of the configured properties and to quickly change the theme settings for each element on a whim.\nRoadblocks 😢 However, when attempting to create the YAML feature, I had noticed a huge flaw in the delegate property system used for configuring the style components. Since the YAML formatting had to be rendered during runtime and as such, the YAML had to modify the existing style which while possible using the Singleton pattern, certain components like relying on a fallback would not be able to register the changes made. Therefore, I had to revert to using a simple class to house the CSS properties of an element and convert the Settings singleton to be a regular object that has to be passed to each element for the configuration to take place.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class CssProperty\u0026lt;T\u0026gt;( val theme: Settings.Theme, private var light: T? = null, private var dark: T? = light, private var fallback: T? = null ) { var value: T? get() = when(theme) { LIGHT -\u0026gt; light DARK -\u0026gt; dark } ?: fallback set(input) { light = input dark = input } } Then in order for the converter to apply the changes of the YAML to the document being generated, we iterate over every element within the style and apply the changes accordingly.\n1 2 3 fun \u0026lt;T\u0026gt; List\u0026lt;Element\u0026gt;.updateStyles(input: T?, update: Element.(T) -\u0026gt; Unit) { input?.let { forEach { e -\u0026gt; e.update(it) } } } The lesson learnt here is that while a structure might look good on paper and work for a specific use case, this might not always be the case and this can result in rewriting of the codebase. I was lucky that the codebase was one that I was very familiar with and I could afford the rewrite to get this new feature up and running. However, I would not always be so lucky and might encounter a codebase that might take too long to rewrite.\n","date":"Jul 13","permalink":"https://blog.woojiahao.com/post/kotlin-delegate-properties/","tags":["Kotlin","tutorial","guide","delegates","kMD2PDF"],"title":"Applications of Kotlin's delegate properties"},{"categories":null,"contents":"kMD2PDF now finally uses FlexMark as the back end for the markdown to html conversion. This is a big moment as now a lot more flexibility has been introduced for the library and that means more features.\nFollowing my promise of making smaller and more frequent release updates, this port has ushered in version 0.2.1 of kMD2PDF and I\u0026rsquo;m really excited!\nChangelog Backend library using flexmark Unit testing framework set up for testing node rendering Task list items now supported Original design Originally, the library used commonmark to handle .md to .html conversions, but it was severely limited as the number of useful extensions was severely lacking and it resulting in a terrible codebase. However, with FlexMark, this problem is alleviated as it has all the features I need.\nHiccups There was a slight hiccup with the port as the .html to .pdf library, flyingsaucer would not accept the HTML output produced by FlexMark. So I had to make use of Jsoup to parse the output of FlexMark to become valid XML that the flyingsaucer library would accept.\nAnother issue faced is the node renderers and visitors. In commonmark, these node renderers were a single class that would be added the parser (seen here) and visitors would be accepted after the document is parsed (seen here). This made it really easy to create custom node renderers and visitors which were used for figure generation and table of contents processing. However, with FlexMark, due to the increase in flexibility, the overhead for creating both increased as well and this resulted in requiring a parsing extension to be created, which would create a custom NodeRenderingFactory which in turn be responsible for creating custom NodeRenderers to render the needed node, which in this case was the figure elements. Whilst this may seem all complicated, it was actually outlined in their sample repository where I was able to successfully create the figure renderer here.\nThe table of content processor was similar in nature. Due to the increased flexibility offered by FlexMark, additional steps had to be taken to create a visitor to properly create the table of contents. This highlighted the idea that developing flexible software would often entail having to increase the overhead of the software because it just takes that many extra steps to provide that flexibility.\nUnit testing the DOM I also worked on attempting to create a system to unit test the node rendering aspect of the library since there can be a lot of edge cases involved with markdown and it might be useful to have an automated system to be able to ensure that the code is reliable and performs within my range of expectation.\nI tried to design this API to be as seamless as possible, reducing the moving parts exposed to the user so that they would not have to fiddle with too many configurations to get it working. What I came up with was rather interesting. To ensure that the rendering was correct, I had to first find a way to test that the converted markdown file would produce a certain result. To do so, I exposed the HTML conversion process of MarkdownConverter to be able to hook into this using the API.\nThe essence of the API is to compare the processed HTML and an expected HTML input using Jsoup to ensure that the they are the same. This required some basic recursion to assert that every single node matched.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 fun assertMarkdown(folder: String, file: String) { require(file.indexOf(\u0026#34;.\u0026#34;) == -1) { \u0026#34;File should not include extensions as they are added within the method\u0026#34; } val markdownFileName = \u0026#34;$file.md\u0026#34; val htmlFileName = \u0026#34;$file.html\u0026#34; val markdownFile = getResource(folder, markdownFileName) val htmlFile = getResource(folder, htmlFileName) val converter = setupConverter(markdownFile) val expectedDocument = parseDocument(htmlFile.readText()).body() val actualDocument = parseDocument(converter.generateBody()) .getElementsByClass(\u0026#34;content\u0026#34;) .first() // Ensure that they both have the same number of children assertEquals(expectedDocument.childCount, actualDocument.childCount) // Ensure that they both have the same set of elements val expectedDocumentBody = expectedDocument.children() val actualDocumentBody = actualDocument.children() expectedDocumentBody.zip(actualDocumentBody).forEach { compare(it.first, it.second) } } As you can see, the markdown file will be the actual output produced by the library whilst the html file represents the expected output.\nThis was the bulk of the API, with the recursive function looking like:\n1 2 3 4 5 6 7 8 9 private fun compare(ex: Element, ac: Element) { assertElementEquals(ex, ac) if (ex.childCount != 0) { (0 until ex.childCount).forEach { compare(ex.child(it), ac.child(it)) } } } This allows the users to simply execute the assertMarkdown() function, providing the resource folder and resource name of the markdown file and html file. This would set in motion an automated set of testing to ensure that every aspect of the generated markdown file would produce the appropriate html.\nOne limitation that is present with the API is that the file name of the markdown file and html file would have to be the same, otherwise the assert function would fail. That said, it also encourages for developers trying to use the API to always stick to the same name for their markdown and html file, which reduces confusion.\n","date":"Apr 08","permalink":"https://blog.woojiahao.com/post/porting-flexmark/","tags":["reflection","migration","Kotlin","markdown","PDF","kMD2PDF","unit testing"],"title":"Porting to FlexMark"},{"categories":null,"contents":"Oh boy, I screwed up bad, like really badly. Ok ok, let\u0026rsquo;s go back and see where it all began. I\u0026rsquo;ve learnt countless lessons from this and I hope you (reader) will too if you\u0026rsquo;re developing your own software.\nStory time! 2 months ago, I embarked on a journey to write a markdown to PDF converter, called kMD2PDF. The initial architecture was developed such that:\n.md file -\u0026gt; .html file + .css styling -\u0026gt; .pdf In order to facilitate this system, I used 2 libraries:\ncommonmark-java - this facilitated with the .md -\u0026gt; .html conversion flyingsaucer - this facilitated with the .html -\u0026gt; .pdf conversion In between, I used my own code to create a style DSL to generate the CSS styling. This system worked nicely as I was able to create an easy to use API for developers using this library.\n1 2 3 4 5 6 7 fun main() { val document = MarkdownDocument(\u0026#34;~/Desktop/README.md\u0026#34;) val convert = markdownConverter { document(document) } converter.convert() } Failure to research However, I made several big oversights. I failed to plan that the libraries I used were severely limited in what they could offer. For instance, commonmark-java does not support task list items, even as an extension and in order to implement this feature, I had to create a custom NodeRenderer that would convert bullet lists to task list items if necessary. Then I realised that because I overrode the default rendering behavior of the bullet lists, I no longer could create nested lists, so I had to remedy that myself and soon, the project became a bunch of band-aids stuck on top of the library and it caused the project to steer into a direction of just bug fix upon bug fix, as I attempted to introduce features into a tightly-coupled system. This made for an incredibly hard time working on the library as I was de-motivated to implement features since they would result in a mess.\nWorst still, as I researched more libraries for handling markdown parsing, I noticed that there were libraries like Flexmark that did provide the support for the features I wanted such as task lists without requiring a lot of hacky work to be performed to the existing library.\nSimilarly, a big hurdle I had to cross with flyingsaucer was getting HTML5 xml code to render as the library required only XML or XHTML documents. This was a huge bottleneck because now some elements render incorrectly and others require even longer HTML that isn\u0026rsquo;t always necessary. That\u0026rsquo;s where I discovered OpenHTMLToPDF, which had a similar API to flyingsaucer but it allowed for HTML5 code and it doesn\u0026rsquo;t constrict users by forcing them to use XHTML (for the uninitiated, XHTML is a stricter form of HTML, where single-enclosed tags are not permitted, rather, every tag must be closed off with an ending tag).\nThis failure to plan ahead and research properly made my life hell as I spent most of my time fixing issues I created for myself.\nOver committing Another issue I had stupidly created for myself is over-committing to creating a huge feature push. Initially, after releasing version 0.1.2, I started work on version 0.2.0 - where many new features would come, along with a set of changes to the existing API to improve the lives of developers. But\u0026hellip; that\u0026rsquo;s where I failed to realise that I had bitten off more than I could chew.\nFor an entire month, I focussed my efforts on trying to make version 0.2.0 feature complete. And within this period of time, the stable version of the library never once got updated. This spelt bad news for those who are using 0.1.2 as they are waiting for over a month for a newer library, whilst they\u0026rsquo;re stuck with a bug-filled library. It also meant that I would burn out quickly working on the library as I tried pushing to complete the features by an arbitrary deadline.\nThis caused me to be extremely stressed when users of the library would hound me for changes. However, during this month, I did learn a lot about Kotlin and software development so it wasn\u0026rsquo;t a complete waste of time. I just know it could have been so much better and smoother.\nThis is where I also begin to see the value of Agile development, where we should break up the development into deliverables across weeks to reduce the workload and to improve the end user experience.\nNo testing An area lacking in the library is unit testing. As the markdown can have many corner cases, it\u0026rsquo;s hard to cater to each of these. The lack of unit tests meant that the bugs I encounter are always by accident, rather than the result of methodical checking and testing to ensure that nothing slips the radar.\nBacktracking to unit test everything is also a pain as by this point, I would have forgotten about some of the classes.\nLearning points So what exactly did I take away from this experience and how will I be improving my approach. I\u0026rsquo;ve learnt to be more methodical with my research before starting a project, I should carefully scope for the best libraries and options available for the job. I\u0026rsquo;ve also come to realise just how key portioning work and creating frequent deliverables is to developing good software. Finally, I\u0026rsquo;ve learnt to use TDD or at least, unit test my code as I write them, rather than leaving it till the last minute.\nSo what exactly does this mean for kMD2PDF? Well, I\u0026rsquo;ll begin by announcing several changes to my workflow. First and foremost, I\u0026rsquo;ll be porting my codebase to use Flexmark and OpenHTMLToPDF over the next few releases, so that\u0026rsquo;s exciting! Secondly, I\u0026rsquo;ll shorten the release cycle to have new versions of the library be released every 2 weeks where the versioning will go from 0.2.0 to 0.2.1 etc. Each release will contain a few features and fixes and the goal is to make the library stable and ready to use as frequently as possible. Lastly, I\u0026rsquo;ll begin to rolling out a suite of unit tests for the codebase. I\u0026rsquo;ll adopt TDD in my workflow and hopefully, I can reach \u0026gt; 80% code coverage before version 1.0.0.\nWhilst I\u0026rsquo;m incredibly frustrated about my set backs, I will not allow them to hinder the development and progress of the library and with a new found resolve to achieve better things, I\u0026rsquo;ll be taking on each new challenge with pride. I have also greatly enjoyed the past month learning various skills and practicing Kotlin.\nIf you\u0026rsquo;re curious about the library, it can be found here.\n","date":"Apr 03","permalink":"https://blog.woojiahao.com/post/lessons-on-software-release/","tags":["reflection","Kotlin","markdown","PDF","converter","software engineering","kMD2PDF"],"title":"Lessons on software release"},{"categories":null,"contents":"Generics was a feature introduced in Java 5 and it has changed how Java developers write code.\nThis article will be discussing some of the things I\u0026rsquo;ve picked up as I dug into generics in Java.\nUses of generics Pre-generics era Imagine you wish to create a container to store an object that can be of any type, pre-generics, you would need to do something like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Container { private Object obj; public Object getObj() { return obj; } public void setObj(Object obj) { this.obj = obj; } } Container container = new Container(); container.setObj(\u0026#34;Hello\u0026#34;); System.out.println((String) container.getObj()); // Prints \u0026#34;Hello\u0026#34; container.setObj(1); System.out.println((Integer) container.getObj()); // Prints 1 As you can see, this is not type safe as you can put anything into the Container and have it spit out something because the Container doesn\u0026rsquo;t care for the type of the inputs. You can get around this lack of type safety by giving the class type of the intended inputs into the Container constructor and check that against the inputs, but that will be extra code for something that seems so logical.\nGenerics era With generics, you can now create the Container with type safety built in and no more casting.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Container\u0026lt;T\u0026gt; { private T obj; public T getObj() { return obj; } public void setObj(T obj) { this.obj = obj; } } Container\u0026lt;String\u0026gt; container = new Container\u0026lt;\u0026gt;(); container.setObj(\u0026#34;Hello\u0026#34;); System.out.println(container.getObj()); // Prints \u0026#34;Hello\u0026#34; container.setObj(1); // Does not compile because container can only receive Strings What\u0026rsquo;s going on? This was a question I had asked a lot prior to reading up, because it was such a mysterious process that I could somehow enforce type safety just with this construct.\nThe type safety generics offer is limited to just compile time, this is because of a process called Type Erasure where the generic type is removed during runtime. This is done in an effort to ensure backward compatibility to pre-generic code.\nThis not only means that the type safety isn\u0026rsquo;t extended to runtime, it also means that any operations requiring the use of this generic type at runtime is not permitted because the type will be erased by that point. For example:\n1 2 3 \u0026lt;T\u0026gt; void foo() { System.out.println(T.class); // This does not work! } Java isn\u0026rsquo;t able to recognize the data type of T during runtime thanks to erasure and thus this line throws and exception. This is especially troubling as you might want to be able to cast the data types from one generic type to another, but this is not possible.\nThis state can also be referred to as generics being non-reified, meaning, it is not present during runtime.\nKotlin to the rescue! Since Kotlin is based off the JVM, it also inherits this type erasure behavior of generic types, which means behavior with generics you see in Java will be the same in Kotlin. However, Kotlin offers a means of enabling a generic type to be preserved during runtime, thus removing the \u0026ldquo;burden\u0026rdquo; imposed on it.\nReified generic types + inline functions In Kotlin, the inline keyword, makes it so the function is not expanded into a separate object but rather \u0026ldquo;copy-pasted\u0026rdquo; into the call site of the function, thus allowing some interesting behavior to occur.\nOne such intriguing behavior is allow generic types to be reified, or retained at runtime. Because the function is now inline, the generic type given to the function can be preserved, thus, allowing you to access the generic type information during runtime.\n1 2 3 fun \u0026lt;T\u0026gt; foo() { println(T::class.java) } Conclusion Generics are an interesting construct with a lot of thought put into designing them.\nSome articles that might peak your interest:\nreified keyword in Kotlin Type Erasure Generics: How They Work And Why They Are Important ","date":"Nov 26","permalink":"https://blog.woojiahao.com/post/java-generics/","tags":["Java","generics","sharing"],"title":"An investigation into generics in Java"},{"categories":null,"contents":"Kotlin is a language that was built on top of the JVM and what really sets it apart from Java is the emphasis on functions being a first-class construct, meaning many fancy things like lambdas are directly available to Kotlin developers without having to include bulky constructs like the concept of functional interfaces.\nWhat are lambdas? In my article on Diving Into Streams, I explained how lambdas were this miraculous constructs in Java that removed a lot of the boilerplate required when passing behavior from one method to another.\nHowever, this definition was really under the assumption of pure Java. Lambdas, as a whole, are a mathematical concept. In programming, a lambda can be seen as a \u0026ldquo;portable\u0026rdquo; function or a piece of behavior that can be passed around and used accordingly.\nThen, what are functions? Functions are simply pieces of behavior that take in some input (or none at all) and return some output (or none at all) , this is very much the same as the mathematical concept it is based off of.\nA case for lambdas Scenario: Say you were working in a performance-critical situation and you just discovered that there is a major memory issue with a function and it is causing some major performance drawbacks and you want to see how long it takes to benchmark and fix this issue.\n\u0026ldquo;No biggie, I can take the time before and after the execution and check how long that took to benchmark the function\u0026rdquo; would be something you are thinking to yourself and that would be true in a simple case like ours.\nYou begin to write some code to test this function:\n1 2 3 4 5 6 7 8 9 10 11 12 fun foo(upper: Int): Int { var sum = 0 for(i in 1..upper) sum += i return sum } fun main(main: Array\u0026lt;String\u0026gt;) { val before = System.currentTimeMillis() println(\u0026#34;foo returns: ${foo(10000)}\u0026#34;) val after = System.currentTimeMillis() println(\u0026#34;foo took ${after - before}ms to run\u0026#34;) } Awesome! It worked, you can now go on your merry way and fix the bug. Then you manager comes around and tells you yet another performance issue was found and you think \u0026ldquo;Eh, this is kind of tedious to redo so I guess I could copy-paste the code I made earlier\u0026rdquo;.\nNow, would you be able to continually repeat this if you suddenly had a whole class of functions that needed to be tested?\nLambdas to the rescue! Going back to how I mentioned how a lambda is a \u0026quot;portable\u0026quot; function or a piece of behavior that can be passed around and used accordingly.\nIn this scenario, we ideally, would want to have a function that can time any function with a method signature like foo() and this timing function would not need to know anything about the function it receives, all it needs to know it that it should calculate the time before and after this function took to run and print that out, nicely formatted.\nSome basic pseudo-code would look like this:\nDetails Input {::nomarkdown}Name of the functionFunction to be timed{:/} Output None, this function will make use of a side effect Steps {::nomarkdown}Take the time before running the functionRun the functionTake the time after running the functionPrint out the difference between the before and after, this will be total time it took to for the function to run{:/} Defining structure In Kotlin, in order to declare a lambda, you have to declare a function type, or some kind of contract for the functions to be passed to follow. To simplify things, all the memory leaks that have surfaced all take in exactly 1 Int input and return an Int, so the function type for this case would look something like:\n1 func: (Int) -\u0026gt; Int Note how the function type is literally just the method signature of a potential function without the parameter names. As mentioned previously, these function types are merely just contracts, it defines what a function to be passed in should look like, it does not dictate what the inputs will be used for (generally what the parameter names represent) nor does it dictate the specific behaviors of this function.\nCreating the timing function Now that we have the general structure of the function we want to be able to time, we can then proceed with declaring the function body of this timer.\n1 2 3 4 5 6 fun functionTimer(name: String, upper: Int, func: (Int) -\u0026gt; Int) { val before = System.currentTimeMillis(); println(\u0026#34;$name returned ${func(upper)}\u0026#34;) val after = System.currentTimeMillis(); println(\u0026#34;$name took ${after - before}ms to run\u0026#34;) } As you can see, this function\u0026rsquo;s body is very much the same as the original method of timing the function in the main method. However, whilst there were some similarities, the benefits of creating a specialized function for timing is as follows:\nModularity - your code is now modular and can be reused if needed Flexible - because this is a function, you can apply consistent changes to the timing and the changes made will be reflected in any function timed by this function made Abstraction - a new user of your codebase does not need to think about how a function is timed, all they need to know is how to call this timing function and they know that this function will help to time and do everything for them Consistency - we can remove the upper argument and apply a consistent argument to be passed to func so that the timing is not skewed by changes in the upper, thus ensuring a degree of consistency Timing things With the newly made timing function, we can then begin to time how long each function took in our class. Let\u0026rsquo;s begin with the first function we tested, which simply added a bunch of numbers from 1 up till an upper value.\n1 2 3 4 5 6 7 fun main(args: Array\u0026lt;String\u0026gt;) { functionTimer(\u0026#34;add\u0026#34;, 10000) { var sum = 0 for (i in 1..it) sum += i sum } } What we have used is the lambda syntax Kotlin provides. We declare the arguments of the functionTimer function as per usual, but we then use {} which houses the body of the function. We also use the it keyword, to refer to the one and only input this lambda receives, which we can assume would be the upper limit (in this case, 10000).\nAnother thing to note is we omit the return keyword from the lambda body even though the function type specifies that each function has to return an Int. We simply put the value to be returned on a line of its own and Kotlin is clever enough to know to return this value.\nNow, we can extend this function to work with any other function that takes in a single Int input and return an Int\u0026hellip;\n1 2 3 4 5 6 7 functionTimer(\u0026#34;multiply\u0026#34;, 100) { var result = 1 for (i in 2..it) result *= i result } functionTimer(\u0026#34;wacky\u0026#34;, 1000) { (1..it).map { num -\u0026gt; num * 3 }.sum() } Sample output:\nadd returned 50005000 add took 1ms to run multiply returned 0 multiply took 0ms to run wacky returned 1501500 wacky took 58ms to run You can see how flexible we can make this timer be, taking in all sorts of functions and timing them.\nTiming existing functions Sometimes, you might not want to rewrite a long function just to time it, and that is perfectly fine, you can pass a reference to an existing method for it to be run as such:\n1 functionTimer(\u0026#34;add\u0026#34;, 10000, ::add) Conclusion Lambdas are far more than just useful for timing stuff, it is extremely versatile and Kotlin uses them extensively throughout their language, in Collections to APIs that leverage off the design of lambdas.\nSome resources for further reading:\nKotlin documentation Baeldung ","date":"Oct 28","permalink":"https://blog.woojiahao.com/post/kotlin-lambdas/","tags":["Kotlin","lambda","functional programming"],"title":"Familiarising yourself with lambdas in Kotlin"},{"categories":null,"contents":" What are streams? Streams was introduced in Java 1.8 and it had completely changed how we write code. The majority of what I will be discussing will be what I have learnt from watching this talk by Venkat Subramaniam. His talk was what had originally got me into using streams and the concept of lambdas.\nLet\u0026rsquo;s revise How do we implement a lambda in Java? Lambdas are simple constructs with very powerful use cases in Java and many other languages. Most commonly, lambdas enable the everyday programmer to reduce their clunky anonymous inner classes into simple one-liners. It can also be used to pass methods around between methods without having to redeclare these method over and over again.\nThe common components that make up a lambda in Java are:\nA functional interface A method that matches the signature of the method in the functional interface What is a functional interface? A functional interface is an interface that contains a single method.\n1 2 3 public interface StringOp { String perform (String in); } Using a lambda: Code:\n1 2 3 4 5 6 7 8 9 10 11 public interface StringOp { String perform (String in); } public class LambdaDemo { public static void main (String[] args) { StringOp operation = in -\u0026gt; new StringBuilder(in).reverse().toString(); System.out.println(operation.perform(\u0026#34;Hello World\u0026#34;)); } } Output:\n1 dlroW olleH In this example, I created a functional interface (StringOp), declared an instance of that functional interface (operation) and gave it\u0026rsquo;s definition all in one line using a lambda.\nThe core syntax of the lambda is as such:\n(parameters) -\u0026gt; { actions } In cases where there is only 1 parameter, the parantheses can be omitted, as seen in the example, and if the method body a single line, you can also omit the curly braces.\nIf lambdas did not exist, I would have to declare the method like this:\n1 2 3 4 5 6 7 8 StringOp operation = new StringOp () { @Override public String perform (String in) { return new StringBuilder(in).reverse().toString(); } }; operation.perform(\u0026#34;Hello World\u0026#34;); These are just simple examples of what lambdas are capable of, there a many more uses for them and you can check out a more comprehensive guide here: https://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html\nStarting streams I will first begin by showing an example of a typical program and then showing the power of streams and how they can be used to simplify your work.\nProblem: Write a program to print out all numbers that are multiples of a given number within a given range.\nTraditional Solution:\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class Solution { static void printMultiples (int multiple, int upper) { for (int i = 1; i \u0026lt;= upper; i++) { if (i % multiple == 0) { System.out.println(i); } } } public static void main (String[] args) { printMultiples(2, 10); } } Streams Solution:\n1 2 3 4 5 6 7 8 9 10 11 12 public class Solution { static void printMultiples (int multiple, int upper) { IntStream .rangeClosed(1, upper) .filter(i -\u0026gt; i % multiple == 0) .forEach(System.out::println); } public static void main (String[] args) { printMultiples(2, 10); } } Output:\n2 4 6 8 10 As you can see, both methods produce the same output, however, the latter is a lot neater than the former. Not only is the solution simpler to understand than the solution that introduces loops and if statements, it is a lot easier to read and understand.\nExplanation: The reason why methods like .filter(i -\u0026gt; i % multiple == 0) works is due to the use of functional interface as mentioned previously. According to the Java documentation on streams, .filter() receives a Predicate interface as a parameter.\nRepresents a predicate (boolean-valued function) of one argument.\nThis means in order to create a lambda that receives one argument and returns a boolean condition.\nMethod references Another unusual syntax you might have noticed is this forEach(System.out::println), you might be scratching your head and wondering that this :: symbol is doing. Well, it is known as a method reference. The core idea with method references would be as Mr. Venkat put it\nSince the value is a simple pass over, you can use a method reference.\nTo illustrate this, let\u0026rsquo;s see how you would use the .forEach() method normally:\n1 2 3 4 5 String[] menu = { \u0026#34;Pizza\u0026#34;, \u0026#34;Cola\u0026#34;, \u0026#34;Salad\u0026#34; }; Arrays.asList(menu) .stream() .forEach(menuItem -\u0026gt; System.out.println(menuItem)); Output:\nPizza Cola Salad As you can see, for forEach(), the menuItem argument you receive is simply being passed onto the System.out.println method call, and since no other modification is being made to this menuItem value, you can use a method reference to System.out.println to shorten to code.\nIn this particular instance, since println is a static method of the System.out object, the method reference will be a reference to a static method, which means the syntax would simply be having the object name followed by the :: symbol and then the target method name.\n","date":"Apr 08","permalink":"https://blog.woojiahao.com/post/diving-into-streams/","tags":["guide","tutorial","Java 8","streams","lambda","functional programming","functional interfaces"],"title":"Diving into (Java) streams"},{"categories":null,"contents":"","date":"Jan 01","permalink":"https://blog.woojiahao.com/articles/","tags":null,"title":"Articles"}]