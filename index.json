[{"categories":null,"contents":"This open-source deep dive has been split into two parts! The first part covers the prerequisite knowledge that would be good to know when trying to understand the inner workings of Broadway. The second part is an in-depth analysis of the implementation of various features of Broadway.\nThis is the first part of the deep dive and the following topics will be covered:\nA brief introduction to what Broadway is Message queues Concurrency in Elixir Producer/consumer model \u0026amp; GenStage Architecture of a Broadway pipeline Construction of producer \u0026amp; processor components If you wish to jump right into the meat of Broadway, you can find the second part here!.\nAct 1, Scene 1 You have just received your latest feature to work on and it is to build a system that receives transaction information from a message queue, maps the customer code in this transaction information to the customer\u0026rsquo;s information, and stores this collective information in a separate database to be queried for customer transaction analysis. Your boss has developed an obsession with Elixir recently and is now pushing for every project to use it. Gasp.\nYou start researching for libraries that can do exactly that and stumble upon Broadway.\n\u0026hellip;build [concurrent] and [multi-stage] [data ingestion] and [data processing] [pipelines]\u0026hellip;\nOh boy\u0026hellip; that — that is a mouthful\u0026hellip; Let\u0026rsquo;s break it down, shall we?\nconcurrent - having two or more computations in progress at the same time; in progress meaning that they do not have to be executed at the same time ( definition here) multi-stage - successive operating stages ( definition here) data ingestion - process of moving data from one source to a destination for further storage and analysis ( definition here) data processing - conversion of data into a usable and desirable form ( definition here) pipelines - series of data processing elements ( definition here) In essence, Broadway builds systems that behave like factory assembly lines. Raw materials (data) is fed into the assembly line (Broadway pipeline) which is then pieced together to create the end product or other components used in the final product. The factory has multiple identical assembly lines running so raw material can be fed into any of these lines to be worked on.\nFor your use case, the flow of data will look something like this:\nSo how does Broadway achieve all of this?\nLights! Camera! Action! Before understanding the internals of Broadway, we should establish some basic knowledge of the technologies we will be using so that we won\u0026rsquo;t be headless chickens running into this.\nBroadway revolve around the following concepts:\nmessage queues concurrency in Elixir What are message queues? Note! While Broadway can integrate with many types of data sources, the core examples given in the project focus on message queues as the primary data source.\nMessage queues are like containers that hold sequences of work objects — called messages — that are to be consumed and processed. It aids with building asynchronous modular and concurrent systems.\nMessages are created and delivered to these queues by producers and taken from these queues for processing by ** consumers.** These messages can vary from something as simple as plain information to more complex structures like requests or — in our case — transaction information.\nMessage queues are useful for decentralising the communication mechanism of large systems by acting as a medium for exchanging events between systems which allows for systems to be easily scaled and distributed.\nThis is a reduced explanation of what a message queue is and what it is capable of. For more information about message queues, the Amazon documentation and this blog post by CloudAMQP are good places to start.\nConcurrency in Elixir Broadway relies heavily on concurrency in Elixir. The topology (architecture) of a pipeline is built on top of processes and many of the features are achieved using the robust concurrency model of Elixir. So what exactly is the concurrency model in Elixir?\nElixir employs the actor concurrency model. In this model, actors are defined as self-isolated units of processing. In Elixir, these actors are called processes and they are managed by the Erlang VM . Elixir code is run in each process and a default/main process is akin to that of the main thread in other concurrency models.\nEach process communicates via asynchronous message passing. Think of a process as a mailbox of sorts; it has a \u0026ldquo;bin\u0026rdquo; to receive incoming messages and it possess an \u0026ldquo;address\u0026rdquo; for other processes to identify it by.\nThe unique aspect of this model is the lack of shared mutable state that other concurrency models rely on. Rather, state is exclusive to each process.\nIn order for the state of a process to be altered, the owner process must make the alteration either on request or internally due to certain changes.\nThe topic of concurrency in Elixir is vast and Elixir provides many other features surrounding its concurrency model such as GenServer. This section is a short preview of what the actor concurrency model and concurrency in Elixir is all about. For more information, you can refer to this thesis paper and the Wikipedia article talking about the actor concurrency model and the official documentation and this tutorial on OTP in Elixir for more examples of concurrency in Elixir.\nCue the producer/consumer model Using the actor concurrency model as a foundation, another concurrency pattern can be modelled in Elixir — the producer/consumer model.\nThis model aims to allow for decoupled data production and consumption by setting up two separate processes to handle each task — effectively creating a logical separation of concerns.\nHowever, the producer/consumer model faces a critical issue — what happens if the producer generates excessive messages for the consumer? The consumer will be overwhelmed and will eventually fail trying to keep up with processing that many messages. This is where back pressure comes into play.\nBack pressure is a control mechanism for how much a producer should emit based on consumer demand, consumer message buffering, or limited sampling\nBack pressure avoids the problem of overloading the consumer with messages by applying one of or a combination of the three methods mentioned above (more information in the link here).\nThe next frontier (of concurrency): GenStage Seeing the value of having a standard implementation for the producer/consumer model, the Elixir team decided to develop exactly that.\nGenStage is a specification for exchanging events between producers and consumers with back pressure between Elixir processes\nProducers emit events to consumers for processing. The events can be of any structure.\nThe control mechanism used is a demand system. Consumers inform producers of how many events they can handle (demand) and producers emits no more than the demanded amount. This ensures that the consumers are capable of handling the events emitted.\nProducer-consumers behave like both producers and consumers. They are used to perform transformations on events emitted by the producer before they are emitted to the consumer.\nSimilar to GenServer, stages in GenStage exchange events through callbacks.\nWhen a demand is handled — i.e. producer emits events and demanding consumer handles these events — another demand is made, creating a cycle where both stages are always working - ideally.\nGenStage is a powerful tool in an Elixir developer\u0026rsquo;s arsenal. More information can be found in the official announcement where a little bit of history of how GenStage came to be was discussed and in a talk by José Valim — creator of Elixir.\nWith a better grasp of the overarching concepts used in Broadway, we can finally discuss what Broadway is all about and how it does what it does!\nPipeline architecture It is at this juncture where it would be important to clarify the term \u0026ldquo;producer\u0026rdquo;. In both message queues and GenStage, a producer is a creator of messages or events. However, in Broadway, a producer is both a consumer of messages and an emitter of events.\nFor the rest of the article, the following definitions for the following terminology will be used:\nproducer — producer of events in Broadway message — message in a message queue or any other data source event — GenStage events When messages are consumed by the producer, they will be transformed into events with a fixed structure defined by Broadway.\nEach component is a separate process and they are dynamically generated as different topologies (architectures) can be designed. The order of initialisation for a typical pipeline looks something like this:\nThe producers and processors are both created using interesting conventions that is will be explored now. Other components will be discussed later on as they tie into other features Broadway has.\nHow it\u0026rsquo;s made: Producers Producers are built using a pattern similar to the strategy pattern but modified to integrate with the concurrency system in Elixir.\nDifferent data sources require different methods of establishing connections and receiving messages. Thus, we break up the producer process into two modules — ProducerStage defines the behavior for enforcing the rate limit while a dynamically loaded module defines the behavior for establishing a connection to the data source and receiving messages.\nProducerStage assumes that the dynamically loaded module contains the typical GenStage callbacks like handle_call and handle_demand and uses them for things like rate limiting.\nThe ProducerStage behaves as the context while the dynamic module behaves as the strategy. The dynamic module adopts the Producer module — which defines two callbacks for managing the overall producer life-cycle.\nTo load the module dynamically, the module name is passed to ProducerStage as an argument. To keep the producer as a single process, we call the init function of the module directly when initialising the ProducerStage. This way, the module will initialise under the newly spawned process for ProducerStage rather than spawning an entirely new process.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @impl true def init({args, index}) do {module, arg} = args[:module] # ... state = %{ module: module, module_state: nil, transformer: transformer, consumers: [], rate_limiting: rate_limiting_state } # Calling the init function of the dynamically loaded module case module.init(arg) do # ... end end When start_link is called, a new process is spawned first before the init function is called under the new process.\nThis is done as certain message queue providers like RabbitMQ attach active listeners to the calling process so spawning a separate process for this would mean having to manage two separate processes for a producer.\nHow it\u0026rsquo;s made: Processors Processors are created using a concept similar to inheritance in object-oriented programming. This idea comes from the need to standardise the subscription logic of producer-consumers and consumers.\nWhen a processor is started using start_link, a process of the Subscriber module is started with the current processor module passed as a argument.\n1 2 3 4 5 6 7 8 9 def start_link(args, stage_options) do Broadway.Topology.Subscriber.start_link( __MODULE__, args[:producers], args, Keyword.take(args[:processor_config], [:min_demand, :max_demand]), stage_options ) end The current module is initialised in the Subscriber process through init.\n1 2 3 4 5 6 @impl true def init({module, names, options, subscription_options}) do {type, state, init_options} = module.init(options) # ... end Other producer-consumers and consumers like batcher and batch processors also use this pattern to create their respective GenStage stages.\nA separation of concern is achieved using this pattern. The processor is responsible for event handling while the subscriber handles the subscription logic.\nThat\u0026rsquo;s a basic rundown of the concepts underpinning Broadway. While it may not be a complete and intensive explanation of everything, hopefully it is able to provide some clarity. In the next part, we will be exploring how features in Broadway have been implemented!\nHop on over to the second part here!\nOpen-source Deep Dive is a series where I pick apart open-source projects to explain the underlying concepts that power these projects and share my findings about the project!\n","date":"Apr 12","permalink":"https://woojiahao.com/blog/post/odd-broadway-1/","tags":["Open-source Deep Dive","Elixir","Broadway","data processing","message queue","message queues","concurrency","actor concurrency model","producer/consumer model","open-source","open-source project"],"title":"Open-source Deep Dive: Broadway (Part 1) - Message queues, concurrency in Elixir, and Broadway architecture"},{"categories":null,"contents":"This open-source deep dive has been split into two parts! The first part covers the prerequisite knowledge that would be good to know when trying to understand the inner workings of Broadway. The second part is an in-depth analysis of the implementation of various features of Broadway.\nThis is the second part of the deep dive and the following topics will be covered:\nRate limiting Batching messages Telemetry Creating a built-in testing support for pipelines Achieving graceful shutdowns Other interesting bits of code If you want a refresher on the concepts behind Broadway (like message queues and concurrency in Elixir) or to better understand Broadway\u0026rsquo;s pipeline architecture from a bird\u0026rsquo;s eye view, you can find the first part here!\nWhat\u0026rsquo;s the scoop? Now that we have explored the overall architecture of a Broadway pipeline, we can look at how certain features in Broadway are implemented.\nRate limiting Rate limiting refers to the act of limiting the amount of data that can be requested or processed in a given period of time\nRate limiting is applied across producers within a single pipeline to control the number of events emitted within a given period of time.\nThis is especially useful when the hardware of the machine running the pipeline is not able to keep up with processing large numbers of events demanded at a time — possibly due to a poorly configured pipeline.\nSome producers do not leverage the rate limiting feature of Broadway. For instance, the RabbitMQ producer creates an active listener, which means that event emission is not inhibited by the rate limiter. Instead, events are emitted the moment messages are published to the message queue ( unless otherwise configured) .\nBut for the producers that do leverage the rate limiting — such as the Amazon SQS producer — rate limiting is applied in two instances:\nWhen consumers make demands to the producer-consumer or producer\nIf the producer can still emit events, any demand made by the consumer will be handled by the producer. We take into account the rate limit threshold. If there are too many events to emit, the excess messages are stored in a message buffer that will have to be cleared later on.\nEach message that can be emitted will be transformed into the standard event structure that Broadway uses.\nIf the producer can no longer emit messages, any demand made is stored in a demand buffer that is cleared later on.\nWhen the rate limit is being reset after the given interval\nAfter the given interval, the rate limit threshold can be reset. However, we may have accumulated demands and messages in their respective buffers. We may find that the threshold has not been met before we reset it. Thus, we can use this remaining threshold to clear any lingering demands and messages stored in their respective buffers.\nOnce we have cleared as many messages as our remaining threshold allows, we will reset the threshold and schedule for another reset. These resets are scheduled at fixed intervals.\nThe rate limiting threshold is maintained as an atomic (discussed later on). This atomic array is generated by the RateLimiter process. This module handles all behavior surrounding working with the rate limit threshold. ProducerStage handles the actual logic of managing the demands of consumers.\nWhen the producer cannot emit any more events, i.e. the threshold has been reached, an internal state is set to :closed to avoid future demands from being handled.\nBatching Batching groups events based on given properties and sends them to designated \u0026ldquo;sub-pipelines\u0026rdquo; or batch processors to be handled. For instance, we might design a pipeline that stores events with even numbers in an S3 bucket and ones with odd numbers on Google Drive.\nThe Batcher process is assigned unique names for identification and events that are emitted from the producer must be tagged to a batcher. Failure to do so will result in a runtime error. This only applies if batching is enabled.\nIn order for the producer to send the appropriate events to the respective batcher, a PartitionDispatcher is used. Essentially, it defines the behavior of how events are emitted to consumers. A PartitionDispatcher dispatches events to certain consumers based on a given criteria (defined as a hash function). In this case, the criterion is the name of the batcher from the given event. This means that when we assign a batcher to the event, it will be dispatched to only that batcher. More information about dispatchers in GenStage can be found in the official documentation.\nEven within the batcher, further grouping can be made based on a batch key assigned to the event. This may be used to ensure that certain events are processed together. Internally, the batcher will accumulate events before emitting them. However, as it cannot sit around accumulating events forever, a batch is emitted at regular intervals regardless of how many events are stored in it.\nThe BatchProcessor process handles a single batch at a time. It is similar to a regular processor, except it works on a batch of events. The handle_batch callback is used here.\nTelemetry Telemetry is used in Broadway to benchmark certain operations that occur such as the duration that a handle_message callback takes.\nBroadway relies on the telemetry library. Within the code, events are emitted when these operations occur and key measurements such as duration are tracked. Handlers/listeners of these events can be setup to respond to these events.\nTelemetry is not an Elixir-only feature. It is commonly used to perform application monitoring. OpenTelemetry is a really interesting framework that offers powerful application monitoring through telemetry.\nBuilt-in testing To test the pipeline, we should focus on ensuring that the data processing aspect of the pipeline works as intended. However, as we rely on external services for input, it would be hard to coordinate a test suite to work with a live data source as we may not be able to replicate the data source or publish data to the data source at will due to access limitations. Thus, Broadway has designed a testing utility that allows us to test the pipeline\u0026rsquo;s data processing capacity without relying on the data source.\nBroadway provides a placeholder producer module. This producer does not rely on any data sources. Instead, messages are emitted directly into the pipeline.\nThe producer module should be tested separately if there is core behavior that cannot be tested along with the pipeline.\nThis form of unit testing ensures that we reduce potential points of failure in our test suite if any of the aforementioned problems with using the original data source should surface.\nGraceful shutdowns Broadway boasts about having graceful shutdowns. This is a rather interesting concept to explore as it relies heavily on the concurrency system of Elixir.\nEssentially, the pipeline can only exist in two states — when all components are online and when all components are shutting down. There is no point in time where a single component will shutdown on its own without being restarted. This is because of the way that the supervisor of each component declares restart strategies ensuring that should a child process encounters any errors, it will be restarted without a hitch. This way, the only time where our components can shut down is when we shut down our main process or pipeline supervisor process. When either process is terminated, we want to properly handle all remaining events in the pipeline before shutting off every component.\nThis is achieved through a mix of concurrency features. But before we can explain how it works, a simple introduction of exit signals and process termination is due.\nExit signals and process termination Processes can be linked to one another. When either process receives an exit signal — which can occur when the process is terminated forcibly or when it receives an exit signal propagated from its parent — it will propagate the exit signal to the linked process and that process will terminate as well.\nHowever, these exit signals can be trapped instead. When this occurs, rather than terminating the process that receives the propagated exit signal, the exit signal is sent as a message, allowing the receiving process to handle the exit as though it was just another message.\n1 2 3 def handle_info({:EXIT, from, reason}, state) do # ... end When a process is terminated, an optional terminate/2 callback can be declared to perform any cleanup before the process is actually terminated. This is useful if we have any lingering operations that should be completed before we terminate the process.\nSupervisors can start a list of child processes and is responsible for managing the restart strategy of each child. The interaction between a supervisor and terminate is rather interesting. When a child is terminated, it is restarted accordingly. When a supervisor terminates, all of its children will also be terminated. If a child process traps exits, the terminate callback is called. If not, it will simply terminate immediately without calling the callback. More information about how supervisor interact with shutdowns can be found in the official documentation.\nBack to our regularly scheduled deep dive\u0026hellip; With a basic understanding of exit trapping and process termination, we can actually understand how graceful shutdowns in Broadway works.\nWhen the main process or the pipeline supervisor process is terminated, the main process — which traps exit signals — will invoke its terminate callback which will inform the Terminator process to begin trapping exits and terminate our pipeline supervisor. As this Terminator process is a child of the pipeline supervisor, it will invoke its implementation of terminate.\n1 2 3 4 5 6 7 8 9 10 11 12 @impl true def terminate(reason, %{name: name, supervisor_pid: supervisor_pid, terminator: terminator}) do Broadway.Topology.Terminator.trap_exit(terminator) ref = Process.monitor(supervisor_pid) Process.exit(supervisor_pid, reason_to_signal(reason)) receive do {:DOWN, ^ref, _, _, _} -\u0026gt; :persistent_term.erase(name) end :ok end The exit signal propagates to the other components through their supervisors terminating and they will also invoke their terminate callback if they trap exits such as producers disconnecting from the data source.\nThe Terminator process is responsible for ensuring that all events still within the pipeline are processed before terminating the pipeline entirely.\nIt does so in three phases:\nNotify that the processors do not resubscribe to producers through a state flag Drain the producers of any events remaining by emitting the events through the pipeline Wait for the batch processors (which will be the very last component in the pipeline) to terminate before terminating the supervisor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @impl true def terminate(_, state) do for name \u0026lt;- state.first, pid = Process.whereis(name) do send(pid, :will_terminate) end for name \u0026lt;- state.producers, pid = Process.whereis(name) do Broadway.Topology.ProducerStage.drain(pid) end for name \u0026lt;- state.last, pid = Process.whereis(name) do ref = Process.monitor(pid) receive do {:done, ^pid} -\u0026gt; :ok {:DOWN, ^ref, _, _, _} -\u0026gt; :ok end end :ok end Interestingly, as the producer may be waiting to drain events, we may not want to cancel all of its consumers immediately. Thus, we rely on GenStage#async_info to queue the message to cancel all consumers at the end of the GenStage message queue — effectively waiting for all other events to be processed before cancelling all consumers. If batching is enabled, the processors will also wait for the batches to be processed before cancelling all consumers.\n1 2 3 4 5 6 7 @spec drain(GenServer.server()) :: :ok def drain(producer) do GenStage.demand(producer, :accumulate) GenStage.cast(producer, {__MODULE__, :prepare_for_draining}) # The :cancel_consumers message is added to the end of the message queue GenStage.async_info(producer, {__MODULE__, :cancel_consumers}) end These mechanisms ensure that all events left in the pipeline is properly processed before the pipeline terminates, thus achieving graceful shutdowns.\nFascinating discovery! These are some interesting bits of code that Broadway has.\n__using__ configurations Like other libraries in Elixir, use Broadway is where it all begins. As discussed in the previous open-source deep dive, the behavior of use can be altered by defining the __using__ macro.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 defmacro __using__(opts) do quote location: :keep, bind_quoted: [opts: opts, module: __CALLER__.module] do @behaviour Broadway @doc false def child_spec(arg) do default = %{ id: unquote(module), start: {__MODULE__, :start_link, [arg]}, shutdown: :infinity } Supervisor.child_spec(default, unquote(Macro.escape(opts))) end defoverridable child_spec: 1 end end There are three interesting bits of code in the __using__ macro:\nlocation: keep\nUsed to report runtime errors from inside the quote. Without this, errors are reported where the defined function ( in quote) is invoked. This is to ensure that we are aware of where the errors are occurring. More information about this configuration can be found here.\nbind_quoted\nUsed to create bindings within the quote. When a binding is created, the value is automatically unquoted (which includes evaluation) and the value cannot be unquoted again. This is especially used when we do not want to re-evaluate the value multiple times.\nMore information quoting and unquoting in Elixir can be found in the official tutorial and a simplified explanation and example of binding can be found here.\n@behaviour\nUsed to define interface-like behavior where modules that adopt these behaviors can implement callbacks defined. In this case, when a module use Broadway, it will have to implement certain callbacks like handle_message while other callbacks like handle_batch remain optional.\n1 2 3 4 5 6 7 8 9 10 11 12 @callback prepare_messages(messages :: [Message.t()], context :: term) :: [Message.t()] @callback handle_message(processor :: atom, message :: Message.t(), context :: term) :: Message.t() @callback handle_batch( batcher :: atom, messages :: [Message.t()], batch_info :: BatchInfo.t(), context :: term ) :: [Message.t()] @callback handle_failed(messages :: [Message.t()], context :: term) :: [Message.t()] @optional_callbacks prepare_messages: 2, handle_batch: 4, handle_failed: 2 More information about typespecs can be found in the official documentation .\nModule metadata processing While on the topic of meta-programming, module metadata can also be processed.\nensure_loaded? ensures that a given module is loaded. In Broadway, this is used to ensure that the :persistent_term module from Erlang is available for Elixir — the only time it will not be available is when the version of Elixir is too old. Documentation here.\n1 2 3 4 5 unless Code.ensure_loaded?(:persistent_term) do require Logger Logger.error(\u0026#34;Broadway requires Erlang/OTP 21.3+\u0026#34;) raise \u0026#34;Broadway requires Erlang/OTP 21.3+\u0026#34; end function_exported? returns whether a module contains a definition for a public function with a given arity. Used to execute functions from modules if they are defined. Documentation here.\n1 2 3 4 5 6 7 8 if Code.ensure_loaded?(producer_mod) and function_exported?(producer_mod, :prepare_for_start, 2) do case producer_mod.prepare_for_start(module, opts) do {child_specs, opts} when is_list(child_specs) -\u0026gt; {child_specs, NimbleOptions.validate!(opts, Broadway.Options.definition())} other -\u0026gt; # ... Dynamic process naming As the pipeline can comprise of any number of components, Broadway supports dynamically generated processes. These dynamically generated processes are assigned names that follow a fixed convention — comprising of the name of the pipeline, the process type, and the index of the component among the other components of the same type.\n1 2 3 4 5 6 7 8 9 defp process_name(prefix, type, index) do :\u0026#34;#{name_prefix(prefix)}.#{type}_#{index}\u0026#34; end defp process_names(prefix, type, config) do for index \u0026lt;- 0..(config[:concurrency] - 1) do process_name(prefix, type, index) end end The names are returned as quoted atoms where the atom has a space in it so it has to be declared via :\u0026quot;\u0026quot; .\nStorage options in Elixir Besides the basic data structures like lists and dictionaries, Elixir and Erlang offer other ways of storing data within processes.\nAtomics\n:atomics are a way of performing atomic operations on a set of mutable atomic variables.\nUsed to maintain the rate limiting threshold.\nPreviously, the rate limiter used ETS instead but atomic operations are much better for concurrent systems as they avoid race conditions when multiple producer processes are attempting to modify the rate limit.\n1 2 counter = :atomics.new(@atomics_index, []) :atomics.put(counter, @atomics_index, allowed) Persistent term\nStorage for Erlang terms that is optimised for reading terms at the expense of writing and updating terms.\nUsed to store pipeline metadata like producer names etc.\n1 2 3 4 5 6 7 8 :persistent_term.put(config.name, %{ context: config.context, producer_names: process_names(config.name, \u0026#34;Producer\u0026#34;, config.producer_config), batchers_names: Enum.map(config.batchers_config, \u0026amp;process_name(config.name, \u0026#34;Batcher\u0026#34;, elem(\u0026amp;1, 0))), rate_limiter_name: config.producer_config[:rate_limiting] \u0026amp;\u0026amp; RateLimiter.rate_limiter_name(opts[:name]) }) Queue\nManage first-in, first-out queues.\nUsed to manage message and demand buffers in the producer.\n1 2 3 4 # A queue of \u0026#34;batches\u0026#34; of messages that we buffered. message_buffer: :queue.new(), # A queue of demands (integers) that we buffered. demand_buffer: :queue.new() Process dictionaries\nStore state within a process directly although its usage is generally frowned upon .\nUsed to store batches in the batcher.\n1 2 3 4 5 6 7 8 9 10 11 defp init_or_get_batch(batch_key, state) do if batch = Process.get(batch_key) do batch else # ... end end defp put_batch(batch_key, {_, _, _} = batch) do Process.put(batch_key, batch) end A better alternative might have been to use an Agent or ETS instead.\nEdit! I clarified with the team about their decision to use process dictionaries over ETS, this was their response:\nThe correct solution here would be to simply use a map. But because this is very intensive code, we need a mutable option, and the process dictionary is the most efficient one. ETS would be slow as data has to be copied in and out of ETS. This is one of the very cases where using the pdict for performance is justified. :)\nSo, the reason why they decided to use a process dictionary over ETS is due to the performance requirement of batching! Very interesting!\nOptions validation Dashbit — the team behind Broadway — developed an options validation library called NimbleOptions that aims to be a small library for validating and documenting high-level options.\nA set of definitions for the available options are created first and these can be used to validate a keyword list — aka the options.\nIf the options are invalid, an error is returned, otherwise an :ok status along with the options are returned. The returned options have default values filled in.\nDefault values in dictionaries Broadway has an interesting way of fanning out default values for the options keyword list. In the options keyword list, a \u0026ldquo;parent\u0026rdquo; value for :partition_by, :hibernate_after, and :spawn_opt is provided.\n1 2 3 4 5 6 7 options = [ partition_by: ..., # these are parent values hibernate_after: ..., producer: [ hibernate_after: ... # this is a child value ] ] The parent value will be used for producers, processors, and batchers if no explicit child value is provided. Alternatively, we might want to fan out a parent value to only two of the three unset child values while maintaining the original child value of the third.\nThis is done by merging the child options into the parent options. Thus, if the child does not define a value for the option, the parent value is inherited.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 opts = opts |\u0026gt; carry_over_one(:producer, [:hibernate_after, :spawn_opt]) |\u0026gt; carry_over_many(:processors, [:partition_by, :hibernate_after, :spawn_opt]) |\u0026gt; carry_over_many(:batchers, [:partition_by, :hibernate_after, :spawn_opt]) defp carry_over_one(opts, key, keys) do update_in(opts[key], fn value -\u0026gt; Keyword.merge(Keyword.take(opts, keys), value) end) end defp carry_over_many(opts, key, keys) do update_in(opts[key], fn list -\u0026gt; defaults = Keyword.take(opts, keys) for {k, v} \u0026lt;- list, do: {k, Keyword.merge(defaults, v)} end) end Closing the curtains To conclude, Broadway is a powerful library for building data processing pipelines. These pipelines are built on top of the robust concurrency system that Elixir boasts.\nBroadway is a very versatile library and the documentation contains detailed guides about using it with various data sources. Check out the Github repository and documentation!\nIf you want to get a basic understanding of the underlying concepts of Broadway or better visualise the architecture of a pipeline in Broadway, check out the first part here!\nOpen-source Deep Dive is a series where I pick apart open-source projects to explain the underlying concepts that power these projects and share my findings about the project!\n","date":"Apr 12","permalink":"https://woojiahao.com/blog/post/odd-broadway-2/","tags":["Open-source Deep Dive","Elixir","Broadway","data processing","message queue","message queues","concurrency","actor concurrency model","producer/consumer model","open-source","open-source project"],"title":"Open-source Deep Dive: Broadway (Part 2) - Inner workings of Broadway"},{"categories":null,"contents":" What is Hound? For browser automation and writing integration tests in Elixir\nLet\u0026rsquo;s inspect this definition a little closer\u0026hellip;\nWhat is browser automation? Browser automation is effectively the process of using a proxy (like Selenium or Hound) to perform browser actions on behalf of the user (like the test case). Essentially, we are automating the usage of the browser.\nIt is often associated with illegal applications like sneaker-botting but much like torrenting, there are positive applications and we will be exploring one of them in this post - integration testing.\nWhat is integration testing? When building software, we first build individual components to support given functional requirements. These individual components can be tested using unit tests - which ensure that given a set of inputs, the component returns a ** predictable** set of outputs (predictable means that the functions tested are pure).\nHowever, while components may work well on their own, when combined with other components (to form larger components/whole systems), unexpected behavior may be exhibited. For instance, the input from component A is transformed before it is used as input to component B, thus, the combined components returns an unexpected result.\nHence, integration tests serve to bridge the gap between individual components testing and full system testing.\nWhen combined with browser automation, we can ensure that a website works end-to-end. We can ensure that the data validation on the front-end works as intended and that the forms submitted by users are properly sent to the back-end and saved in the database.\nApproaching browser automation integration testing\u0026hellip; We can take two approaches to browser automation integration testing. We could either\nbuild our own interfacing system to communicate with the browser, or rely on existing interfacing systems The former is time-consuming and requires a lot of care during development as we have to account for varying browser APIs and quirks. Thus, it is wiser to chose the latter when approaching browser automation integration testing. Doing so minimizes the number of components we have to manage.\nIntroducing Hound! This is where Hound comes into the picture. Hound provides a clean API to build browser automation tests. It relies on Selenium, PhantomJS, and ChromeDriver as the interfacing systems to perform the \u0026ldquo;dirty\u0026rdquo; work of coordinating requests/responses to/from the browser.\nThis introduces a larger question, what exactly is Selenium, PhantomJS, and ChromeDriver? More importantly, in fact,\n\u0026ldquo;How is browser automation performed?\u0026rdquo;\nUnderstanding how browser automation is performed provides us with a better foundation to grasp how these technologies work and how Hound works under the hood.\nThe world of web drivers\u0026hellip; The key driving (pun intended) of browser automation is web drivers. But before we can understand what they are, we should establish some basic understanding of what a driver is in general computing terms.\nWhat are drivers? Drivers are pieces of software that behave as a proxy between a caller and a target. Callers can be something like the print prompt in Google Chrome or a computer peripheral. Targets can be something like the printer or computer.\nIn general, drivers are responsible for translating the caller\u0026rsquo;s request into a given format that the target can understand.\nThere may be variations of a caller to the same target so each driver must be able to translate their respective caller\u0026rsquo;s request into a common request format for the target. For instance, there are multiple types of keyboards that can be connected to a single computer but the computer can only understand a single request format. So the respective keyboard drivers are responsible for converting the unique keyboard\u0026rsquo;s requests into the format that the computer accepts.\nBack to web drivers Similar to general drivers, web drivers behave as proxies for the caller (Hound) to communicate with the target ( browser). It allows the caller to send instructions for the browser to perform. In the web development world, a browser is also referred to as a user agent.\nThe Selenium project proposed a W3C specification to guide the development of web drivers. For the rest of this discussion, we will be relying on this specification. The specification can be found here.\nAccording to the specification, there must exist a separation of concern when designing a web driver. More specifically, there are two components to a web driver:\nLocal end - API for developers to send requests to the browser (libraries like Selenium and Hound) Remote end - responsible for communicating with the browser, i.e. a browser driver (can you infer what this means?) In essence, a web driver is comprised of an API and a browser driver. Ideally, the API should be able to work with different browser drivers for different browsers.\nThe remote end must also provide an HTTP compliant wire protocol where each endpoint maps to a command for the browser.\nThis means that the remote end relies on HTTP to communicate requests with the browser. The remote end is a HTTP server that the local end writes HTTP requests to. The remote end translates each HTTP request (based on endpoint and method) to a command for the browser. Note that a wire protocol is a method of getting data from one point to another. It dictates that requests should follow a given format.\nThe specification also provides an outline for the endpoints that the remote end must make available for the local end. This ensures standardization and ease of adoption for future browser drivers.\nOne advantage to using a HTTP server for the remote end is that it is possible to host the remote end on a remote machine. This means that we can delegate the job of integration testing to another machine, a process commonly known as distributed testing. By enabling distributed testing, the local machine is not burdened with the responsibility of testing potentially extensive and rigorous integration tests which the machine may not support.\nSo, how does Selenium work? Selenium implements the web driver specification (they did author it). The remote end uses the JSON Wire Protocol as the HTTP compliant wire protocol to communicate with the browser driver. Note that the documentation provided by Selenium (for the JSON Wire Protocol) has been obsoleted in favor of the one defined in the specification.\nHow does Selenium differ from PhantomJS? PhantomJS is a headless browser library. Headless browsers are essentially web browsers without the graphical interfaces. Selenium, on the other hand, is a web driver. The key difference between the two is the way requests are routed and managed (PhantomJS is a rather interesting project so Open-source Deep Dive: PhantomJS edition maybe?).\nHowever, Selenium supports headless browsers as well and more importantly, Selenium is still in active development while PhantomJS has been archived due to a lack of active contributions.\nWhat is ChromeDriver then? ChromeDriver is a browser driver developed as part of the Chromium project. It is used by Selenium as one of the supported browser drivers. However, Hound supports raw requests to ChromeDriver as the underlying HTTP server works the same with or without the use of Selenium. It is an interesting project so I may explore it in another installment of Open-source Deep Dive!\nHound: Under the hood Leveraging browser drivers With a better understanding of how browser automation and web drivers work, we can in fact see that Hound doesn\u0026rsquo;t rely on the entirety of Selenium (including the local end APIs). Instead, it relies on the remote end of the Selenium web driver (along with PhantomJS and ChromeDriver) to minimize the number of \u0026ldquo;moving components\u0026rdquo; that need to be managed while reaping the benefits of the existing technologies. Thus, it can focus on delivering a seamless API for developing browser automation integration tests.\nExploring a basic use case We will inspect a basic use case of Hound before diving into how Hound works.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 defmodule HoundTest do use ExUnit.Case use Hound.Helpers hound_session() test \u0026#34;the truth\u0026#34;, meta do navigate_to(\u0026#34;https://google.com\u0026#34;) element = find_element(:class, \u0026#34;search\u0026#34;) fill_field(element, \u0026#34;Apples\u0026#34;) submit_element(element) assert page_title() == \u0026#34;Apples\u0026#34; end end There is quite a bit to unpack here. Let\u0026rsquo;s first understand the core of how a test suite with Hound is setup.\nFirst, we use ExUnit.Case as Hound works hand in hand with ExUnit, a built-in Elixir library for developing unit tests. It relies on two components of ExUnit: setup and on_exit. This allows Hound to work as expected.\nThen, we use Hound.Helpers which, with the power of macros, imports all helper functions that are required to access the browser session.\nFinally, we call hound_session() which creates a new session (an instance of the browser) and initializes the setup and tear down functionality of a Hound browser automation test.\nOnce the core of the browser automation test is built, we can write test cases as per normal, leveraging on functions like navigate_to() and fill_field() to perform browser actions. The intended behavior of these functions are easy to understand and the documentation for them can be found here.\nBreaking it down With a basic understanding of how a test suite can be setup in Hound, we can start to decompose Hound to better understand what makes it tick.\nFirst, we need to inspect the following file: lib/hound/helpers.ex which houses the Hound.Helpers module, the same one that we use in the example above.\nBy overriding the __using__ macro, Hound is able to import all of the helper functions into a given file with a single use statement. This helps to minimize the boilerplate for users to get started. Macros are meta programming constructs that inject code during compile-time. More on macros here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 defmacro __using__([]) do quote do import Hound import Hound.Helpers.Cookie import Hound.Helpers.Dialog import Hound.Helpers.Element import Hound.Helpers.Navigation import Hound.Helpers.Orientation import Hound.Helpers.Page import Hound.Helpers.Screenshot import Hound.Helpers.SavePage import Hound.Helpers.ScriptExecution import Hound.Helpers.Session import Hound.Helpers.Window import Hound.Helpers.Log import Hound.Helpers.Mouse import Hound.Matchers import unquote(__MODULE__) end end Hound.Helpers also defines the hound_session() function which relies on setup and on_exit() of ExUnit to setup and tear down a session between every test case.\n1 2 3 4 5 6 7 8 9 10 11 defmacro hound_session(opts \\\\ []) do quote do setup do Hound.start_session(unquote(opts)) parent = self() on_exit(fn -\u0026gt; Hound.end_session(parent) end) :ok end end end Each helper function constructs a HTTP request to the browser driver server using Hackney . For instance, navigate_to - which opens a given URL in the session - creates the following HTTP request:\n1 2 3 4 5 def navigate_to(url, retries \\\\ 0) do final_url = generate_final_url(url) session_id = Hound.current_session_id make_req(:post, \u0026#34;session/#{session_id}/url\u0026#34;, %{url: final_url}, %{}, retries) end Hound rolls its own HTTP request/response management system that supports multiple retries. This can be found in the lib/hound/request_utils.ex file.\nWe have managed to break down the core functionality of Hound. There are additional interesting components to Hound that I would like to explore as well.\nProcesses Applications are started according to standard OTP specification (here). lib/hound.ex starts a link to Hound.Supervisor which initializes two workers: Hound.ConnectionServer and Hound.SessionServer. These are child processes (Hound isn\u0026rsquo;t fully up-to-date with Application convention) that the supervisor manages.\n1 2 3 4 5 6 7 8 def init([options]) do children = [ worker(Hound.ConnectionServer, [options]), worker(Hound.SessionServer, []) ] supervise(children, strategy: :one_for_one) end Let\u0026rsquo;s explore what the connection server and session server are all about next.\nMore information on processes in Elixir here.\nConnection server This process is responsible for managing the details of the browser driver and providing information to construct the HTTP server endpoints. It stores the driver information using Agent to allow the information to be accessed across processes.\nMore information on Agent here.\nSession management Sessions, as mentioned earlier, refer to instances of the browser that we want to run our tests on. As Hound supports multiple sessions across different processes, it has rolled a session management system.\nSession management in Hound relies on ETS, a built-in storage option provided by Erlang and available in Elixir. When the session server first starts, it creates a new ETS table to hold the session information. This server is setup as a GenServer which allows it to support asynchronous and synchronous callbacks from other processes.\n1 2 3 4 def init(state) do :ets.new(@name, [:set, :named_table, :protected, read_concurrency: true]) {:ok, state} end When a session is first created by hound_session(), the current process\u0026rsquo;s ID (by default, it\u0026rsquo;s the main process) is related to the session. The process is monitored and the new session is created. Under the process ID, multiple sessions can be created, thus allowing Hound to support multi-session testing. Each session is identified by an ID. Each session is also assigned a name. By default, we use the session name of :default. The ETS table holds the following information (mapped to JSON for illustration purposes. In reality, ETS tables store tuples of data so the actual data stored does not include any keys, just the values in the given order):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [ { // Process ID \u0026#34;pid\u0026#34;: ..., // Process monitoring ref \u0026#34;ref\u0026#34;: ..., \u0026#34;session_id\u0026#34;: ..., // Map containing all sessions running \u0026#34;session\u0026#34;: { \u0026#34;\u0026lt;session_name\u0026gt;\u0026#34;: \u0026#34;\u0026lt;session_id\u0026gt;\u0026#34;, ... } } ] As you can see, a session name can be assigned to the same session ID, but not the other way around (I am not too sure why this is setup as such, more investigation would be required).\nWith the ETS table setup to manage session information, we can avoid a major problem: passing around the session ID to various functions. If we had done so, we would have increased the overhead required when using the session ID as functions would have to be designed to accept the session ID and we would have to devise a method of passing the session ID around.\nInstead, the session ID is retrieved from the server on demand using the current_session_id() function in lib/hound.ex.\nAs the current session ID is related to the calling process ID, multiple processes can have different sessions, thus, providing multi-session testing support. This also means that if the calling process changes, the associated session will be retrieved or a new session will be created dynamically.\nIf a process dies - i.e. Process.monitor() sends a DOWN message - the session server will destroy all associated sessions with that process asynchronously.\n1 2 3 4 5 6 def handle_info({:DOWN, ref, _, _, _}, state) do if pid = state[ref] do destroy_sessions(pid) end {:noreply, state} end Configurations Configurations are managed using Elixir\u0026rsquo;s Config API which uses keyword parameter lists to manage configurations.\n1 2 3 4 import Config config :hound, browser: \u0026#34;firefox\u0026#34; The configurations are stored as application environment variables which are retrieved by the connection server.\n1 driver = options[:driver] || Application.get_env(:hound, :driver, \u0026#34;selenium\u0026#34;) More information on the Config API here.\nCoding conventions Other rather interesting bits of Elixir convention that Hound employs are:\n^ (pin) operator\nThe pin operator ensures that a variable, when matched during assignment, is the same as the existing variable of the given name\nIn Hound, this is used to ensure that the retrieved process ID of session (from the session server) is the same as the given process ID (from argument).\nIf the retrieved pid does not match the pid argument, an error is raised.\n1 2 3 4 5 6 def all_sessions_for_pid(pid) do case :ets.lookup(@name, pid) do [{^pid, _ref, _session_id, all_sessions}] -\u0026gt; all_sessions [] -\u0026gt; %{} end end More information on the pin operator here.\ndefdelegate\ndefdelegate dictates that a function\u0026rsquo;s underlying behavior is deferred to that of another function in another module.\nThis allows a module to house the functionality of different modules without breaking the modularity afforded by the module system.\nInterestingly, the __using__ override in Hound.Helpers can be replaced with a multitude of defdelegate to the helper functions but it would, understandably, create a lot of confusion.\nMore information on defdelegate here.\n= (match operator) in function parameters\nAs = is the match operator in Elixir, it can be used to perform pattern matching while assigning the matched pattern to a variable name.\nThis is very useful when working with structures as you may not want to deconstruct the entire structure while ensuring that arguments follow the given structure.\n1 2 3 def foo(%User{} = user) do IO.puts user[:name] end More information on the match operator here.\nPattern matching as enums\nPattern matching with atoms can be used as substitutes for typical enum behavior.\nAn enum in Kotlin may look like:\n1 2 3 4 5 6 7 enum class MatchClause(val name: String) { CLASS(\u0026#34;class\u0026#34;), CSS(\u0026#34;css selector\u0026#34;), NAME(\u0026#34;name\u0026#34;), ID(\u0026#34;id\u0026#34;), ELEM(\u0026#34;element\u0026#34;) } In Elixir, it can be written as such:\n1 2 3 4 5 def match(:class), do: \u0026#34;class\u0026#34; def match(:css), do: \u0026#34;css selector\u0026#34; def match(:name), do: \u0026#34;name\u0026#34; def match(:id), do: \u0026#34;id\u0026#34; def match(:elem), do: \u0026#34;element\u0026#34; Conclusion To conclude, Hound is a browser automation and integration testing library built on top of web driver - more specifically, browser driver - technologies as it leverages Selenium, PhantomJS, and ChromeDriver to build a highly abstracted and simple to use API for building integration tests.\nUnder the hood, Hound is an intriguing project that uses fundamental constructs to build powerful internal libraries that support complex operations.\nIf you are interested in the topics discussed in this post, here are some additional readings:\nWhat is a web driver in Selenium? W3C WebDriver Specification Hound GitHub repository Hackney GitHub repository Distributed testing What is PhantomJS? ChromeDriver repository Uses of browser automation BitTorrent protocol ExUnit Application conventions in Elixir Process monitoring ETS GenServer NOTE: I do not condone the use of browser automation or torrenting for illegal purposes. Any links or discussions about the mentioned subjects are purely for educational purposes and should remain as that.\nOpen-source Deep Dive is a series where I pick apart open-source projects to explain the underlying concepts that power these projects and share my findings about the project!\n","date":"Jan 17","permalink":"https://woojiahao.com/blog/post/odd-hound/","tags":["Open-source Deep Dive","Elixir","Hound","Browser automation testing","Selenium","PhantomJS"],"title":"Open-source Deep Dive: Hound"},{"categories":null,"contents":"Over my time in TPH, I have noticed that a common woe aspiring bot developers have is that they are unable to host their Discord bot online as they may not have access to a credit card.\nIntroducing Heroku! While the official Discord bots used in TPH - like HotBot - is hosted via paid platforms, there are free alternatives to deploying your bot online. This is where Heroku comes into the picture!\nHeroku is a cloud platform that lets companies build, deliver, monitor and scale apps — we\u0026rsquo;re the fastest way to go from idea to URL, bypassing all those infrastructure headaches.\nHeroku\u0026rsquo;s free tier does not require any credit card information and has sufficient uptime for your basic bot development needs and it is a great starting place to understand hosting.\nHow does Heroku work? Before diving into setting up a Discord bot on Heroku, it is best to explain how Heroku is used. Heroku relies on the Git version control system (VCS) to manage an application. This means that it integrates well with any existing projects that already use Git. Do not fret, even if your application does not use Git, the configuration and setup for Heroku is still simple.\nBy using Git, Heroku receives the project files directly and it is responsible for building the project. This is unlike other hosting platforms where you would often only supply the final executable - a .jar file in our case - to the hosting platform to run.\nIn order for Heroku to understand how it will build and deploy your application, you must provide a Procfile.\nThe Procfile is comprised of two key components - the dyno to run the application on and the commands to run your application.\nAccording to the Heroku documentation on dynos, dynos are containers that are used to run and scale all Heroku applications. Rather than worrying about configuring your build environment or OS, you can focus on building your applications and allowing Heroku to take over the build and deployment process. For all Discord bots, we will use a worker dyno.\nThe build commands we supply correspond to the build commands we use to run our bots locally.\nAs Heroku uses the project files to determine the type of tools we are using, we do not need to specify the instructions to create the executable. In our case, since we are using Maven, it can intelligently detect the pom.xml file and create the .jar accordingly. This leaves us with only the run commands to include in our Procfile .\nFinally, to tighten security, we will store all bot tokens in Heroku\u0026rsquo;s config vars. From a code perspective, these config vars are simply environment variables available to our applications. This allows us to load our bot token during runtime and prevent the bot token from being leaked.\nThus, we can define our deployment plan as such:\nInitialise the codebase as a Git repository Create a Heroku application for the bot Create a Procfile to supply instructions for Heroku to run the bot Store the bot token as a config var to be used by your bot What I have just presented is a general overview of Heroku as a hosting platform. I will be diving into the implementation in the following sections.\nGetting started For this article, I will be using a very simple Discord bot written in Kotlin. I have chosen to use JDA as the focus of this guide is to understand Heroku. The code repository can be found here.\nIf you wish to follow along, you can get the repository via\n1 2 $ git clone https://github.com/woojiahao/discord-heroku-deployment-demo ping-bot $ cd ping-bot/ Aside from that, basic understanding of the following is good to have to understand the technical details of this guide.\nGit - version control system that integrates with Heroku to enable easy deployments Maven - build tool for Kotlin to manage application dependencies In Kotlin/Java, we are looking to create a .jar file. This .jar file can be thought of like a .exe file. Essentially, it bundles the application and allows us to run our bot without having to fire up an IDE.\nTo create this .jar file, we will use Maven. For more information about using Maven to create .jar files, refer to this guide.\nWith the formalities out of the way, let\u0026rsquo;s get down to deploying our bot.\nInstalling Heroku You will have to install Heroku onto your machine to execute the following commands in the command line. You can find the installation instructions for Heroku here.\nTo ensure that you have installed Heroku successfully, run heroku --version. My version of Heroku is heroku/7.39.2 linux-x64 node-v13.12.0\nSetup a Git repository As mentioned earlier, we need to ensure that our application is a Git repository for Heroku to work.\nWhile it is recommended to publish your repository to GitHub ( or any other version control website), it is not necessary for deploying your applicaiton to Heroku.\nIf you are using the sample bot, it is already a Git repository.\nIf you are deploying your own bot, initialise a repository by using the following command inside the root folder of your codebase.\n1 $ git init Create a new Heroku application Then, we want to create a Heroku application.\n1 $ heroku create [project name] The project name is optional and will be automatically generated if not provided. It is recommended that you give a name to be organised.\nTo ensure that the Heroku application has been created, run the git remote -v command to list the remotes of your repository. Should your application have been created successfully, you will see a new remote added linking to a Heroku Git remote.\nWith the Heroku application created, we can begin configuring our repository to deploy to Heroku.\nCreating a Procfile As explained earlier, the Procfile acts as a build instruction manual for our application. It instructs Heroku how we want to run our application. Heroku takes over the rest and helps with managing our build environment.\nFor my sample bot, the Procfile looks like this:\nworker: java -jar target/Bot.jar Let\u0026rsquo;s breakdown this file. We first declare the dyno type as worker. Then, we specify the command to run our .jar file.\nHeroku is able to intelligently detect that our Kotlin application uses Maven as a build tool and runs the mvn clean install command to create our Bot.jar file. Then, it will use the commands in the Procfile to run the application.\nSecuring Discord bot tokens A Discord bot requires a token to run.\nYou can obtain this bot token when you make a new Discord bot from the Discord developer dashboard. However, you do not want to expose this token in your repository as this would mean that others could launch and access your bot.\nAs mentioned earlier, we will make use of Heroku\u0026rsquo;s config vars to safely store and access this token.\nWe will add our bot\u0026rsquo;s token as an environment variable and use System.getenv() method to retrieve this value.\n1 $ heroku config:set BOT_TOKEN=\u0026lt;bot token\u0026gt; Inside the Bot.kt file, you will find the following lines in the main() function.\n1 2 val token = System.getenv(\u0026#34;BOT_TOKEN\u0026#34;) ?: throw Exception(\u0026#34;Must include bot token in environment variable for bot to run\u0026#34;) This will retrieve the corresponding environment variable that we have stored in Heroku. If there is no environment variable present, we will stop the bot from launching and display an error.\nAn additional benefit of storing our bot tokens as an environment variable is that we are able to store the bot token locally as an environment variable which streamlines our development process as we could have a separate token used for a development/testing bot.\nLaunching the bot After configuring everything, commit all the changes to your project, and push it to the heroku remote.\n1 2 3 $ git add . $ git commit -am \u0026#34;Setup Heroku\u0026#34; $ git push heroku master If you encounter a problem with pushing to the heroku remote, use the command heroku logs --tail and find the latest error messages to debug any errors.\nAfter pushing the changes, Heroku will build your application. However, it is not online yet as you have to scale your application. This tells Heroku how many instances of your application you wish to run. For our case, we can go with one worker dyno.\n1 $ heroku ps:scale worker=1 You can now invite your bot to a server and test it out. If you\u0026rsquo;re using the sample PingBot, you can use !ping and expect the bot to respond with Pong!.\nNow what? Congratulations! You have just deployed a Discord bot onto Heroku! When you make changes to the bot, you are free to commit and push those changes to the heroku remote to update the bot that is online.\nHere are some tips for developing with Heroku.\nWhile working on your development copy of the bot, it is recommended that you obtain a seprate bot token and attach it as an environment variable to your local development environment. Doing so allows you to maintain your bot\u0026rsquo;s uptime while making changes. If you encounter any errors or your bot is not responding, use the heroku logs --tail command to view the logs of your application. Doing so allows you to check if there were any errors while running your project. If you require persistent storage, Heroku comes with a free tier plugin for PostgreSQL. Heroku - by default - has ephemeral storage, meaning it does not maintain new files after each build. Conclusion Heroku offers a free alternative to many hosting platforms and is a perfect platform for aspiring bot developers to begin.\nMore resources on hosting JVM-based applications on Heroku:\nGetting Started on Heroku with Java Java Sample (on GitHub) ","date":"Apr 21","permalink":"https://woojiahao.com/blog/post/deploying-discord-bot-kotlin/","tags":["Kotlin","Heroku","Discord bot","deployment","tutorial","guide"],"title":"Deploying discord bots written in Kotlin to Heroku"},{"categories":null,"contents":" What are streams? Streams was introduced in Java 1.8 and it had completely changed how we write code. The majority of what I will be discussing will be what I have learnt from watching this talk by Venkat Subramaniam. His talk was what had originally got me into using streams and the concept of lambdas.\nLet\u0026rsquo;s revise How do we implement a lambda in Java? Lambdas are simple constructs with very powerful use cases in Java and many other languages. Most commonly, lambdas enable the everyday programmer to reduce their clunky anonymous inner classes into simple one-liners. It can also be used to pass methods around between methods without having to redeclare these method over and over again.\nThe common components that make up a lambda in Java are:\nA functional interface A method that matches the signature of the method in the functional interface What is a functional interface? A functional interface is an interface that contains a single method.\n1 2 3 public interface StringOp { String perform (String in); } Using a lambda: Code:\n1 2 3 4 5 6 7 8 9 10 11 public interface StringOp { String perform (String in); } public class LambdaDemo { public static void main (String[] args) { StringOp operation = in -\u0026gt; new StringBuilder(in).reverse().toString(); System.out.println(operation.perform(\u0026#34;Hello World\u0026#34;)); } } Output:\n1 dlroW olleH In this example, I created a functional interface (StringOp), declared an instance of that functional interface (operation) and gave it\u0026rsquo;s definition all in one line using a lambda.\nThe core syntax of the lambda is as such:\n(parameters) -\u0026gt; { actions } In cases where there is only 1 parameter, the parantheses can be omitted, as seen in the example, and if the method body a single line, you can also omit the curly braces.\nIf lambdas did not exist, I would have to declare the method like this:\n1 2 3 4 5 6 7 8 StringOp operation = new StringOp () { @Override public String perform (String in) { return new StringBuilder(in).reverse().toString(); } }; operation.perform(\u0026#34;Hello World\u0026#34;); These are just simple examples of what lambdas are capable of, there a many more uses for them and you can check out a more comprehensive guide here: https://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html\nStarting streams I will first begin by showing an example of a typical program and then showing the power of streams and how they can be used to simplify your work.\nProblem: Write a program to print out all numbers that are multiples of a given number within a given range.\nTraditional Solution:\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class Solution { static void printMultiples (int multiple, int upper) { for (int i = 1; i \u0026lt;= upper; i++) { if (i % multiple == 0) { System.out.println(i); } } } public static void main (String[] args) { printMultiples(2, 10); } } Streams Solution:\n1 2 3 4 5 6 7 8 9 10 11 12 public class Solution { static void printMultiples (int multiple, int upper) { IntStream .rangeClosed(1, upper) .filter(i -\u0026gt; i % multiple == 0) .forEach(System.out::println); } public static void main (String[] args) { printMultiples(2, 10); } } Output:\n2 4 6 8 10 As you can see, both methods produce the same output, however, the latter is a lot neater than the former. Not only is the solution simpler to understand than the solution that introduces loops and if statements, it is a lot easier to read and understand.\nExplanation: The reason why methods like .filter(i -\u0026gt; i % multiple == 0) works is due to the use of functional interface as mentioned previously. According to the Java documentation on streams, .filter() receives a Predicate interface as a parameter.\nRepresents a predicate (boolean-valued function) of one argument.\nThis means in order to create a lambda that receives one argument and returns a boolean condition.\nMethod references Another unusual syntax you might have noticed is this forEach(System.out::println), you might be scratching your head and wondering that this :: symbol is doing. Well, it is known as a method reference. The core idea with method references would be as Mr. Venkat put it\nSince the value is a simple pass over, you can use a method reference.\nTo illustrate this, let\u0026rsquo;s see how you would use the .forEach() method normally:\n1 2 3 4 5 String[] menu = { \u0026#34;Pizza\u0026#34;, \u0026#34;Cola\u0026#34;, \u0026#34;Salad\u0026#34; }; Arrays.asList(menu) .stream() .forEach(menuItem -\u0026gt; System.out.println(menuItem)); Output:\nPizza Cola Salad As you can see, for forEach(), the menuItem argument you receive is simply being passed onto the System.out.println method call, and since no other modification is being made to this menuItem value, you can use a method reference to System.out.println to shorten to code.\nIn this particular instance, since println is a static method of the System.out object, the method reference will be a reference to a static method, which means the syntax would simply be having the object name followed by the :: symbol and then the target method name.\n","date":"Apr 08","permalink":"https://woojiahao.com/blog/post/diving-into-streams/","tags":["guide","tutorial","Java 8","streams","lambda","functional programming","functional interfaces"],"title":"Diving into (Java) streams"},{"categories":null,"contents":"","date":"Jan 01","permalink":"https://woojiahao.com/blog/articles/","tags":null,"title":"Articles"}]